# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.spatial_ops import (
    reproject_gdf as reproject_gdf,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layer_from_gdf as create_deckgl_layer_from_gdf,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import get_gdf_geom_type as get_gdf_geom_type

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_line_chart as draw_line_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_cl_ctx_cover as create_cl_ctx_cover,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_context_page_lg as create_context_page_lg,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_vehicles_grouper_ctx as create_vehicles_grouper_ctx,
)
from ecoscope_workflows_ext_lion_guardians.tasks import merge_cl_files as merge_cl_files
from ecoscope_workflows_ext_mnc.tasks import add_totals_row as add_totals_row
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import create_grouper_page as create_grouper_page
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import round_off_values as round_off_values
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            timezone={
                "label": "UTC",
                "tzCode": "UTC",
                "name": "UTC",
                "utc_offset": "+03:00",
            },
            **(params_dict.get("time_range") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(groupers=["subject_name"], **(params_dict.get("groupers") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    base_map_defs = (
        set_base_maps_pydeck.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    persist_ambo_gpkg = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("persist_ambo_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/fcy57d5x5y67gh0xje6rw/lg_group_ranch_boundaries.gpkg?rlkey=yte99xhkte7f1r7n9nemgy7p0&st=sxx1emc1&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_ambo_gpkg") or {}),
        )
        .call()
    )

    persist_hotspot_areas = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("persist_hotspot_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/nlozcti0oqhj6bzvsr46g/lg_conflict_hotspots.gpkg?rlkey=giwdizp1j6e24btgh48uxf5v0&st=jivblame&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_hotspot_areas") or {}),
        )
        .call()
    )

    persist_protected_gpkg = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("persist_protected_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/i5yczgyln3zh1n8c4ppl5/lg_protected_areas.gpkg?rlkey=5ea21haq2tmsmx7g502p3qag5&st=zt6ztcku&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_protected_gpkg") or {}),
        )
        .call()
    )

    load_ambo_group_ranches = (
        load_df.validate()
        .set_task_instance_id("load_ambo_group_ranches")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=persist_ambo_gpkg,
            layer=None,
            deserialize_json=False,
            **(params_dict.get("load_ambo_group_ranches") or {}),
        )
        .call()
    )

    load_hotspot_areas = (
        load_df.validate()
        .set_task_instance_id("load_hotspot_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=persist_hotspot_areas,
            layer=None,
            deserialize_json=False,
            **(params_dict.get("load_hotspot_areas") or {}),
        )
        .call()
    )

    load_protected_areas = (
        load_df.validate()
        .set_task_instance_id("load_protected_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=persist_protected_gpkg,
            layer=None,
            deserialize_json=False,
            **(params_dict.get("load_protected_areas") or {}),
        )
        .call()
    )

    reproject_ambo_boundaries = (
        reproject_gdf.validate()
        .set_task_instance_id("reproject_ambo_boundaries")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_ambo_group_ranches,
            target_crs="EPSG:4326",
            **(params_dict.get("reproject_ambo_boundaries") or {}),
        )
        .call()
    )

    reproject_hotspot_areas = (
        reproject_gdf.validate()
        .set_task_instance_id("reproject_hotspot_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_hotspot_areas,
            target_crs="EPSG:4326",
            **(params_dict.get("reproject_hotspot_areas") or {}),
        )
        .call()
    )

    reproject_protected_areas = (
        reproject_gdf.validate()
        .set_task_instance_id("reproject_protected_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_protected_areas,
            target_crs="EPSG:4326",
            **(params_dict.get("reproject_protected_areas") or {}),
        )
        .call()
    )

    annotate_ambo_layers = (
        get_gdf_geom_type.validate()
        .set_task_instance_id("annotate_ambo_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=reproject_ambo_boundaries,
            **(params_dict.get("annotate_ambo_layers") or {}),
        )
        .call()
    )

    annotate_hotspot_layers = (
        get_gdf_geom_type.validate()
        .set_task_instance_id("annotate_hotspot_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=reproject_hotspot_areas,
            **(params_dict.get("annotate_hotspot_layers") or {}),
        )
        .call()
    )

    annotate_protected_layers = (
        get_gdf_geom_type.validate()
        .set_task_instance_id("annotate_protected_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=reproject_protected_areas,
            **(params_dict.get("annotate_protected_layers") or {}),
        )
        .call()
    )

    custom_amboseli_layer = (
        create_deckgl_layer_from_gdf.validate()
        .set_task_instance_id("custom_amboseli_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=annotate_ambo_layers,
            style={
                "get_line_color": [169, 169, 169],
                "get_fill_color": [169, 169, 169],
                "get_line_width": 4.0,
                "opacity": 0.85,
                "extruded": False,
                "stroked": True,
                "filled": False,
            },
            legend={
                "title": "Map layers",
                "values": [{"label": "Group ranch boundaries", "color": "#a9a9a9"}],
            },
            **(params_dict.get("custom_amboseli_layer") or {}),
        )
        .call()
    )

    custom_hotspot_layer = (
        create_deckgl_layer_from_gdf.validate()
        .set_task_instance_id("custom_hotspot_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=annotate_hotspot_layers,
            style={
                "get_line_color": [220, 20, 60],
                "get_fill_color": [220, 20, 60],
                "get_radius": 2.55,
                "get_line_width": 1.95,
                "opacity": 0.85,
                "extruded": False,
                "stroked": True,
                "filled": True,
            },
            legend={
                "title": "",
                "values": [{"label": "Hotspot areas", "color": "#dc143c"}],
            },
            **(params_dict.get("custom_hotspot_layer") or {}),
        )
        .call()
    )

    custom_protected_layer = (
        create_deckgl_layer_from_gdf.validate()
        .set_task_instance_id("custom_protected_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=annotate_protected_layers,
            style={
                "get_line_color": [77, 102, 0],
                "get_fill_color": [77, 102, 0],
                "get_line_width": 1.95,
                "opacity": 0.2,
                "extruded": False,
                "stroked": True,
                "filled": True,
            },
            legend={
                "title": "",
                "values": [
                    {"label": "National parks and reserves", "color": "#4d6600"}
                ],
            },
            **(params_dict.get("custom_protected_layer") or {}),
        )
        .call()
    )

    subject_obs = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_obs") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_obs,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__subject_subtype",
                "extra__subject__sex",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    persist_relocs = (
        persist_df.validate()
        .set_task_instance_id("persist_relocs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="relocations",
            df=subject_reloc,
            **(params_dict.get("persist_relocs") or {}),
        )
        .call()
    )

    subject_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("subject_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(relocations=subject_reloc, **(params_dict.get("subject_traj") or {}))
        .call()
    )

    persist_trajs = (
        persist_df.validate()
        .set_task_instance_id("persist_trajs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="trajectories",
            df=subject_traj,
            **(params_dict.get("persist_trajs") or {}),
        )
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=subject_traj,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    classify_trajectories_speed_bins = (
        apply_classification.validate()
        .set_task_instance_id("classify_trajectories_speed_bins")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={"label_range": False, "label_decimals": 1},
            **(params_dict.get("classify_trajectories_speed_bins") or {}),
        )
        .call()
    )

    rename_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__name": "subject_name",
                "extra__sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
            },
            df=classify_trajectories_speed_bins,
            **(params_dict.get("rename_traj_cols") or {}),
        )
        .call()
    )

    split_subject_traj_groups = (
        split_groups.validate()
        .set_task_instance_id("split_subject_traj_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            groupers=groupers,
            **(params_dict.get("split_subject_traj_groups") or {}),
        )
        .call()
    )

    sort_trajs_by_speed = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="speed_bins",
            na_position="first",
            ascending=True,
            **(params_dict.get("sort_trajs_by_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    apply_speed_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_speed_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
    )

    format_speed_bin_labels = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_speed_bin_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_bin_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
    )

    format_speed_values = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_speed_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_kmhr",
            output_column_name="speed_kmhr",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=format_speed_bin_labels)
    )

    filter_speed_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_speed_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=[
                "dist_meters",
                "speed_bins_colormap",
                "geometry",
                "speed_kmhr",
                "speed_bins_formatted",
            ],
            **(params_dict.get("filter_speed_cols") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=format_speed_values)
    )

    generate_speedmap_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_speedmap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "speed_bins_colormap",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Speed (km/h)",
                "label_column": "speed_bins_formatted",
                "color_column": "speed_bins_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            **(params_dict.get("generate_speedmap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=filter_speed_cols)
    )

    zoom_speed_gdf_extent = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_speed_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_speed_gdf_extent") or {}))
        .mapvalues(argnames=["gdf"], argvalues=filter_speed_cols)
    )

    combined_ldx_speed_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_speed_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[
                custom_amboseli_layer,
                custom_hotspot_layer,
                custom_protected_layer,
            ],
            **(params_dict.get("combined_ldx_speed_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
    )

    zip_speedmap_with_viewstate = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_speedmap_with_viewstate")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combined_ldx_speed_layers, zoom_speed_gdf_extent],
            **(params_dict.get("zip_speedmap_with_viewstate") or {}),
        )
        .call()
    )

    draw_speedmap = (
        draw_map.validate()
        .set_task_instance_id("draw_speedmap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_speedmap") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zip_speedmap_with_viewstate
        )
    )

    persist_speedmap_html = (
        persist_text.validate()
        .set_task_instance_id("persist_speedmap_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speedmap",
            **(params_dict.get("persist_speedmap_html") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speedmap)
    )

    create_speedmap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speed Map", **(params_dict.get("create_speedmap_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speedmap_html)
    )

    merge_speedmap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_speedmap_widgets,
            **(params_dict.get("merge_speedmap_widgets") or {}),
        )
        .call()
    )

    generate_track_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_track_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": [0, 0, 255],
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Vehicle tracks",
                "values": [{"label": "Tracks", "color": "#0000ff"}],
            },
            **(params_dict.get("generate_track_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=split_subject_traj_groups)
    )

    zoom_track_gdf_extent = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_track_gdf_extent")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_track_gdf_extent") or {}))
        .mapvalues(argnames=["gdf"], argvalues=split_subject_traj_groups)
    )

    combine_track_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combine_track_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[
                custom_amboseli_layer,
                custom_hotspot_layer,
                custom_protected_layer,
            ],
            **(params_dict.get("combine_track_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_track_layers)
    )

    zip_track_layers_view = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_track_layers_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combine_track_layers, zoom_track_gdf_extent],
            **(params_dict.get("zip_track_layers_view") or {}),
        )
        .call()
    )

    draw_track_map = (
        draw_map.validate()
        .set_task_instance_id("draw_track_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            static=False,
            title=None,
            max_zoom=15,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_track_map") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zip_track_layers_view
        )
    )

    track_html_url = (
        persist_text.validate()
        .set_task_instance_id("track_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="tracks",
            **(params_dict.get("track_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_track_map)
    )

    create_tracks_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_tracks_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Vehicle Tracks", **(params_dict.get("create_tracks_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=track_html_url)
    )

    merge_tracks_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_tracks_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_tracks_widgets,
            **(params_dict.get("merge_tracks_widgets") or {}),
        )
        .call()
    )

    draw_speed_line_chart = (
        draw_line_chart.validate()
        .set_task_instance_id("draw_speed_line_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            x_column="segment_start",
            y_column="speed_kmhr",
            category_column=None,
            line_kwargs={"shape": "linear"},
            layout_kwargs={
                "title": None,
                "title_x": 0.01,
                "title_y": 0.95,
                "showlegend": False,
                "fontsize": 20,
                "fontcolor": "#2c3e50",
                "plot_bgcolor": "#f5f5f5",
                "xaxis": {
                    "title": "Date",
                    "showgrid": True,
                    "gridcolor": "#e0e0e0",
                    "gridwidth": 1,
                    "tickformat": "%Y-%m-%d",
                    "tickangle": -45,
                    "rangemode": "auto",
                },
                "yaxis": {"title": "Speed (km/h)", "tickformat": ".1f"},
                "legend_title": None,
                "hovermode": "x unified",
            },
            **(params_dict.get("draw_speed_line_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=split_subject_traj_groups)
    )

    persist_vehicle_chart = (
        persist_text.validate()
        .set_task_instance_id("persist_vehicle_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speed_line_chart",
            **(params_dict.get("persist_vehicle_chart") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speed_line_chart)
    )

    line_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("line_chart_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Vehicle Speed Over Time",
            **(params_dict.get("line_chart_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_vehicle_chart)
    )

    grouped_plot_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_plot_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=line_chart_widget,
            **(params_dict.get("grouped_plot_widget_merge") or {}),
        )
        .call()
    )

    summary_table = (
        summarize_df.validate()
        .set_task_instance_id("summary_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["subject_name"],
            summary_params=[
                {
                    "display_name": "mean_speed",
                    "aggregator": "mean",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "min_speed",
                    "aggregator": "min",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "max_speed",
                    "aggregator": "max",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "total_distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                    "decimal_places": 2,
                },
            ],
            reset_index=True,
            **(params_dict.get("summary_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    add_total_events_row = (
        add_totals_row.validate()
        .set_task_instance_id("add_total_events_row")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            label_col=["subject_name"],
            label="Total",
            **(params_dict.get("add_total_events_row") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    persist_summary_table = (
        persist_df.validate()
        .set_task_instance_id("persist_summary_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename=None,
            **(params_dict.get("persist_summary_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=add_total_events_row)
    )

    calc_mean_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_mean_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="mean_speed", **(params_dict.get("calc_mean_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_mean_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_mean_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_mean_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_mean_speed)
    )

    calc_min_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_min_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="min_speed", **(params_dict.get("calc_min_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_min_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_min_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_min_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_min_speed)
    )

    calc_max_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="max_speed", **(params_dict.get("calc_max_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_max_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_max_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_max_speed)
    )

    total_distance_covered = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_distance_covered")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="total_distance",
            **(params_dict.get("total_distance_covered") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_total_distance = (
        round_off_values.validate()
        .set_task_instance_id("round_total_distance")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_total_distance") or {}))
        .mapvalues(argnames=["value"], argvalues=total_distance_covered)
    )

    total_mean_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_mean_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Mean Speed",
            decimal_places=2,
            **(params_dict.get("total_mean_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_mean_speed)
    )

    total_mean_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_mean_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_mean_speed_widgets,
            **(params_dict.get("total_mean_speed_sv_widget") or {}),
        )
        .call()
    )

    total_min_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_min_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Min Speed",
            decimal_places=2,
            **(params_dict.get("total_min_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_min_speed)
    )

    total_min_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_min_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_min_speed_widgets,
            **(params_dict.get("total_min_speed_sv_widget") or {}),
        )
        .call()
    )

    total_max_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_max_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Max Speed",
            decimal_places=2,
            **(params_dict.get("total_max_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_max_speed)
    )

    total_max_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_max_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_max_speed_widgets,
            **(params_dict.get("total_max_speed_sv_widget") or {}),
        )
        .call()
    )

    total_distance_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_distance_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Distance covered",
            decimal_places=2,
            **(params_dict.get("total_distance_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_total_distance)
    )

    total_distance_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_distance_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_distance_widgets,
            **(params_dict.get("total_distance_sv_widget") or {}),
        )
        .call()
    )

    convert_speed_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_speed_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 20000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_speed_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_speedmap_html)
    )

    convert_tracks_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_tracks_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 20000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_tracks_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=track_html_url)
    )

    convert_line_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_line_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 10,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_line_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_vehicle_chart)
    )

    download_cover_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/huzg95h8p04a1wtgi9nbv/vehicles_cover_page.docx?rlkey=g3a7vi3ncesffww6mfafc9npn&st=jtn3m5jk&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_cover_page") or {}),
        )
        .call()
    )

    download_sect_templates = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_sect_templates")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/hk5gh4kd6uf2cl68dcaac/custom_vehicle_template.docx?rlkey=a2ior7pcmgcb3e8a1d7qchodb&st=wh94dfph&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_sect_templates") or {}),
        )
        .call()
    )

    unique_subjects = (
        dataframe_column_nunique.validate()
        .set_task_instance_id("unique_subjects")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            column_name="groupby_col",
            **(params_dict.get("unique_subjects") or {}),
        )
        .call()
    )

    create_cover_tpl_context = (
        create_cl_ctx_cover.validate()
        .set_task_instance_id("create_cover_tpl_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            count=unique_subjects,
            report_period=time_range,
            prepared_by="Ecoscope",
            **(params_dict.get("create_cover_tpl_context") or {}),
        )
        .call()
    )

    persist_cover_context = (
        create_context_page_lg.validate()
        .set_task_instance_id("persist_cover_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_cover_page,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context=create_cover_tpl_context,
            filename="lg_cover_page.docx",
            **(params_dict.get("persist_cover_context") or {}),
        )
        .call()
    )

    group_context_values = (
        zip_groupbykey.validate()
        .set_task_instance_id("group_context_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[
                split_subject_traj_groups,
                persist_summary_table,
                convert_speed_png,
                convert_tracks_png,
                convert_line_png,
            ],
            **(params_dict.get("group_context_values") or {}),
        )
        .call()
    )

    indv_cl_ctx = (
        create_vehicles_grouper_ctx.validate()
        .set_task_instance_id("indv_cl_ctx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(grouper_name=groupers, **(params_dict.get("indv_cl_ctx") or {}))
        .mapvalues(
            argnames=["df", "summary_table", "speedmap", "tracks_map", "line_chart"],
            argvalues=group_context_values,
        )
    )

    create_grouper_doc = (
        create_grouper_page.validate()
        .set_task_instance_id("create_grouper_doc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_sect_templates,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            validate_images=True,
            box_h_cm=6.5,
            box_w_cm=11.11,
            **(params_dict.get("create_grouper_doc") or {}),
        )
        .mapvalues(argnames=["context"], argvalues=indv_cl_ctx)
    )

    merge_docx = (
        merge_cl_files.validate()
        .set_task_instance_id("merge_docx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=persist_cover_context,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=create_grouper_doc,
            filename=None,
            **(params_dict.get("merge_docx") or {}),
        )
        .call()
    )

    vehicles_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("vehicles_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                total_mean_speed_sv_widget,
                total_min_speed_sv_widget,
                total_max_speed_sv_widget,
                total_distance_sv_widget,
                merge_speedmap_widgets,
                merge_tracks_widgets,
                grouped_plot_widget_merge,
            ],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("vehicles_dashboard") or {}),
        )
        .call()
    )

    return vehicles_dashboard

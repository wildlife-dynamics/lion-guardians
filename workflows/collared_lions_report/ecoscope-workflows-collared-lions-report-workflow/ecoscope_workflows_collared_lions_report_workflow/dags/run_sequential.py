# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import groupbykey as groupbykey
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_lion_guardians.tasks import add_totals_row as add_totals_row
from ecoscope_workflows_ext_lion_guardians.tasks import (
    clean_file_keys as clean_file_keys,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_cover_context_page as create_cover_context_page,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_map_layers as create_map_layers,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_report_context as create_report_context,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    download_file_and_persist as download_file_and_persist,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    draw_custom_map as draw_custom_map,
)
from ecoscope_workflows_ext_lion_guardians.tasks import flatten_tuple as flatten_tuple
from ecoscope_workflows_ext_lion_guardians.tasks import (
    get_split_group_names as get_split_group_names,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    load_geospatial_files as load_geospatial_files,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    make_text_layer as make_text_layer,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    merge_docx_files as merge_docx_files,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    merge_static_and_grouped_layers as merge_static_and_grouped_layers,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    round_off_values as round_off_values,
)
from ecoscope_workflows_ext_lion_guardians.tasks import select_koi as select_koi
from ecoscope_workflows_ext_lion_guardians.tasks import (
    set_custom_base_maps as set_custom_base_maps,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    view_state_deck_gdf as view_state_deck_gdf,
)
from ecoscope_workflows_ext_lion_guardians.tasks import zip_lists as zip_lists

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            timezone={
                "label": "UTC",
                "tzCode": "UTC",
                "name": "UTC",
                "utc_offset": "+00:00",
            },
            **(params_dict.get("time_range") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    base_map_defs = (
        set_custom_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    persist_ambo_gpkg = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_ambo_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/phlc488gxqpcvr6ua3vk7/amboseli_group_ranch_boundaries.gpkg?rlkey=p5ztypwmj4ndjova9xe2ssiun&st=pknuicus&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_ambo_gpkg") or {}),
        )
        .call()
    )

    persist_cover_page = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/kyjd9ii9nul1osbkezl5w/collared_lions_cover_page.docx?rlkey=4nl68thyqzd0n49wnr1770u0x&st=bf5fi1ke&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_cover_page") or {}),
        )
        .call()
    )

    persist_indv_subject_page = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_indv_subject_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/ncwp9obmzbgb847b40d8z/collared_lion_individual_template.docx?rlkey=9mfjjr46ojt3yzena4u8zxuf2&st=vsqac9ny&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_indv_subject_page") or {}),
        )
        .call()
    )

    load_local_shapefiles = (
        load_geospatial_files.validate()
        .set_task_instance_id("load_local_shapefiles")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            config={"path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"]},
            **(params_dict.get("load_local_shapefiles") or {}),
        )
        .call()
    )

    clean_local_geo_files = (
        clean_file_keys.validate()
        .set_task_instance_id("clean_local_geo_files")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_dict=load_local_shapefiles,
            **(params_dict.get("clean_local_geo_files") or {}),
        )
        .call()
    )

    create_custom_map_layers = (
        create_map_layers.validate()
        .set_task_instance_id("create_custom_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_dict=load_local_shapefiles,
            style_config={
                "styles": {
                    "amboseli_group_ranch_boundaries": {
                        "stroked": True,
                        "filled": False,
                        "get_elevation": 50,
                        "opacity": 0.95,
                        "get_line_color": [169, 169, 169, 200],
                        "get_line_width": 3.75,
                    }
                },
                "legend": {"label": ["Group ranch boundaries"], "color": ["#a9a9a9"]},
            },
            **(params_dict.get("create_custom_map_layers") or {}),
        )
        .call()
    )

    filter_aoi = (
        select_koi.validate()
        .set_task_instance_id("filter_aoi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_dict=clean_local_geo_files,
            key_value="amboseli_group_ranch_boundaries",
            **(params_dict.get("filter_aoi") or {}),
        )
        .call()
    )

    custom_text_layer = (
        make_text_layer.validate()
        .set_task_instance_id("custom_text_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            txt_gdf=filter_aoi,
            label_column="R_NAME",
            fallback_columns=["name", "title"],
            use_centroid=True,
            color=[0, 0, 0, 255],
            size=75,
            font_family="Calibri",
            font_weight="bold",
            background=False,
            background_color=None,
            background_padding=None,
            text_anchor="start",
            alignment_baseline="bottom",
            billboard=True,
            pickable=True,
            tooltip_columns=["label"],
            target_crs="epsg:4326",
            **(params_dict.get("custom_text_layer") or {}),
        )
        .call()
    )

    subject_obs = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_obs") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_obs,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__subject_subtype",
                "extra__subject__sex",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    persist_relocs = (
        persist_df.validate()
        .set_task_instance_id("persist_relocs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="relocations",
            df=subject_reloc,
            **(params_dict.get("persist_relocs") or {}),
        )
        .call()
    )

    subject_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("subject_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(relocations=subject_reloc, **(params_dict.get("subject_traj") or {}))
        .call()
    )

    persist_trajs = (
        persist_df.validate()
        .set_task_instance_id("persist_trajs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="trajectories",
            df=subject_traj,
            **(params_dict.get("persist_trajs") or {}),
        )
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=subject_traj,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    rename_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__name": "subject_name",
                "extra__sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
            },
            df=traj_add_temporal_index,
            **(params_dict.get("rename_traj_cols") or {}),
        )
        .call()
    )

    split_subject_traj_groups = (
        split_groups.validate()
        .set_task_instance_id("split_subject_traj_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            groupers=groupers,
            **(params_dict.get("split_subject_traj_groups") or {}),
        )
        .call()
    )

    td = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("td")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("td") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_subject_traj_groups)
    )

    td_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("td_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=td)
    )

    td_map_layer = (
        create_geojson_layer.validate()
        .set_task_instance_id("td_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": "percentile_colormap",
                "opacity": 0.45,
                "get_line_width": 0.75,
                "stroked": True,
            },
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            **(params_dict.get("td_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=td_colormap)
    )

    combine_custom_map_layers = (
        merge_static_and_grouped_layers.validate()
        .set_task_instance_id("combine_custom_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_custom_map_layers, custom_text_layer],
            **(params_dict.get("combine_custom_map_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=td_map_layer)
    )

    zoom_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_view_state") or {}))
        .mapvalues(argnames=["gdf"], argvalues=td_colormap)
    )

    zip_layers_view = (
        groupbykey.validate()
        .set_task_instance_id("zip_layers_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            iterables=[combine_custom_map_layers, zoom_view_state],
            **(params_dict.get("zip_layers_view") or {}),
        )
        .call()
    )

    td_ecomap = (
        draw_custom_map.validate()
        .set_task_instance_id("td_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            static=False,
            title=None,
            max_zoom=15,
            legend_style={"placement": "bottom-right", "title": "ETD Metrics"},
            **(params_dict.get("td_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zip_layers_view)
    )

    td_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("td_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("td_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=td_ecomap)
    )

    td_map_widget = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("td_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(title="Home Range Metrics", **(params_dict.get("td_map_widget") or {}))
        .map(argnames=["view", "data"], argvalues=td_ecomap_html_url)
    )

    td_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("td_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=td_map_widget, **(params_dict.get("td_grouped_map_widget") or {})
        )
        .call()
    )

    summary_table = (
        summarize_df.validate()
        .set_task_instance_id("summary_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["subject_name"],
            summary_params=[
                {
                    "display_name": "mean_speed",
                    "aggregator": "mean",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "min_speed",
                    "aggregator": "min",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "max_speed",
                    "aggregator": "max",
                    "column": "speed_kmhr",
                    "decimal_places": 2,
                },
                {
                    "display_name": "total_distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                    "decimal_places": 2,
                },
            ],
            reset_index=True,
            **(params_dict.get("summary_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    add_total_events_row = (
        add_totals_row.validate()
        .set_task_instance_id("add_total_events_row")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            label_col=["subject_name"],
            label="Total",
            **(params_dict.get("add_total_events_row") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    persist_summary_table = (
        persist_df.validate()
        .set_task_instance_id("persist_summary_table")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_summary_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=add_total_events_row)
    )

    collared_html_png = (
        html_to_png.validate()
        .set_task_instance_id("collared_html_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 20000},
            **(params_dict.get("collared_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=td_ecomap_html_url)
    )

    unique_subjects = (
        dataframe_column_nunique.validate()
        .set_task_instance_id("unique_subjects")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            column_name="groupby_col",
            **(params_dict.get("unique_subjects") or {}),
        )
        .call()
    )

    create_cover_context = (
        create_cover_context_page.validate()
        .set_task_instance_id("create_cover_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            report_period=time_range,
            prepared_by="Ecoscope",
            count=unique_subjects,
            template_path=persist_cover_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="cover_page.docx",
            **(params_dict.get("create_cover_context") or {}),
        )
        .call()
    )

    zip_metrics_etd = (
        groupbykey.validate()
        .set_task_instance_id("zip_metrics_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            iterables=[persist_summary_table, collared_html_png],
            **(params_dict.get("zip_metrics_etd") or {}),
        )
        .call()
    )

    flatten_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=zip_metrics_etd)
    )

    get_grouper_names = (
        get_split_group_names.validate()
        .set_task_instance_id("get_grouper_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            split_data=split_subject_traj_groups,
            **(params_dict.get("get_grouper_names") or {}),
        )
        .call()
    )

    zip_grouper_with_context = (
        zip_lists.validate()
        .set_task_instance_id("zip_grouper_with_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=get_grouper_names,
            right=flatten_context,
            **(params_dict.get("zip_grouper_with_context") or {}),
        )
        .call()
    )

    flatten_final_report_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_final_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_final_report_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=zip_grouper_with_context)
    )

    subject_context_doc = (
        create_report_context.validate()
        .set_task_instance_id("subject_context_doc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=persist_indv_subject_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            validate_images=True,
            box_h_cm=9.0,
            box_w_cm=15.0,
            **(params_dict.get("subject_context_doc") or {}),
        )
        .mapvalues(
            argnames=[
                "grouper_type",
                "grouper_eq",
                "grouper_value",
                "subject_metrics",
                "home_range_ecomap",
            ],
            argvalues=flatten_final_report_context,
        )
    )

    generate_collared_report = (
        merge_docx_files.validate()
        .set_task_instance_id("generate_collared_report")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=create_cover_context,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=subject_context_doc,
            filename="overall_report.docx",
            **(params_dict.get("generate_collared_report") or {}),
        )
        .call()
    )

    calc_mean_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_mean_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="mean_speed", **(params_dict.get("calc_mean_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_mean_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_mean_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_mean_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_mean_speed)
    )

    calc_min_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_min_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="min_speed", **(params_dict.get("calc_min_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_min_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_min_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_min_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_min_speed)
    )

    calc_max_speed = (
        dataframe_column_sum.validate()
        .set_task_instance_id("calc_max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="max_speed", **(params_dict.get("calc_max_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_max_speed = (
        round_off_values.validate()
        .set_task_instance_id("round_max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_max_speed") or {}))
        .mapvalues(argnames=["value"], argvalues=calc_max_speed)
    )

    total_distance_covered = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_distance_covered")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="total_distance",
            **(params_dict.get("total_distance_covered") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summary_table)
    )

    round_total_distance = (
        round_off_values.validate()
        .set_task_instance_id("round_total_distance")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_total_distance") or {}))
        .mapvalues(argnames=["value"], argvalues=total_distance_covered)
    )

    total_mean_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_mean_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Mean Speed",
            decimal_places=2,
            **(params_dict.get("total_mean_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_mean_speed)
    )

    total_mean_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_mean_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_mean_speed_widgets,
            **(params_dict.get("total_mean_speed_sv_widget") or {}),
        )
        .call()
    )

    total_min_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_min_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Min Speed",
            decimal_places=2,
            **(params_dict.get("total_min_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_min_speed)
    )

    total_min_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_min_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_min_speed_widgets,
            **(params_dict.get("total_min_speed_sv_widget") or {}),
        )
        .call()
    )

    total_max_speed_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_max_speed_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Max Speed",
            decimal_places=2,
            **(params_dict.get("total_max_speed_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_max_speed)
    )

    total_max_speed_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_max_speed_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_max_speed_widgets,
            **(params_dict.get("total_max_speed_sv_widget") or {}),
        )
        .call()
    )

    total_distance_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_distance_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Distance covered",
            decimal_places=2,
            **(params_dict.get("total_distance_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_total_distance)
    )

    total_distance_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_distance_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_distance_widgets,
            **(params_dict.get("total_distance_sv_widget") or {}),
        )
        .call()
    )

    lg_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("lg_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                total_mean_speed_sv_widget,
                total_min_speed_sv_widget,
                total_max_speed_sv_widget,
                total_distance_sv_widget,
                td_grouped_map_widget,
            ],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("lg_dashboard") or {}),
        )
        .call()
    )

    return lg_dashboard

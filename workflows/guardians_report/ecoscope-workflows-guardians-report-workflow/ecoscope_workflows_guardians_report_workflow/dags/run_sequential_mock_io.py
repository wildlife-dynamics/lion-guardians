# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª


from ecoscope_workflows_core.tasks.config import set_workflow_details
from ecoscope_workflows_core.tasks.filter import set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps
from ecoscope_workflows_ext_lion_guardians.tasks import load_map_files
from ecoscope_workflows_ext_lion_guardians.tasks import create_map_layers

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import process_relocations
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory,
)
from ecoscope_workflows_core.tasks.transformation import add_temporal_index
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_classification
from ecoscope_workflows_core.tasks.groupby import split_groups
from ecoscope_workflows_core.tasks.transformation import sort_values
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_color_map
from ecoscope_workflows_core.tasks.transformation import map_values_with_unit
from ecoscope_workflows_core.tasks.transformation import map_columns
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polyline_layer
from ecoscope_workflows_core.tasks.skip import any_is_empty_df
from ecoscope_workflows_core.tasks.skip import any_dependency_skipped
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df
from ecoscope_workflows_ext_lion_guardians.tasks import combine_map_layers
from ecoscope_workflows_ext_lion_guardians.tasks import create_view_state_from_gdf
from ecoscope_workflows_ext_lion_guardians.tasks import zip_grouped_by_key
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap
from ecoscope_workflows_core.tasks.io import persist_text
from ecoscope_workflows_core.tasks.results import create_map_widget_single_view
from ecoscope_workflows_core.tasks.skip import never
from ecoscope_workflows_core.tasks.results import merge_widget_views
from ecoscope_workflows_core.tasks.results import gather_dashboard

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .handle_errors(task_instance_id="workflow_details")
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .handle_errors(task_instance_id="time_range")
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .handle_errors(task_instance_id="groupers")
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .handle_errors(task_instance_id="er_client_name")
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    base_map_defs = (
        set_base_maps.validate()
        .handle_errors(task_instance_id="base_map_defs")
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    load_local_shapefiles = (
        load_map_files.validate()
        .handle_errors(task_instance_id="load_local_shapefiles")
        .partial(**(params_dict.get("load_local_shapefiles") or {}))
        .call()
    )

    create_custom_map_layers = (
        create_map_layers.validate()
        .handle_errors(task_instance_id="create_custom_map_layers")
        .partial(
            file_dict=load_local_shapefiles,
            **(params_dict.get("create_custom_map_layers") or {}),
        )
        .call()
    )

    subject_group_observations = (
        get_subjectgroup_observations.validate()
        .handle_errors(task_instance_id="subject_group_observations")
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_group_observations") or {}),
        )
        .call()
    )

    subject_relocations = (
        process_relocations.validate()
        .handle_errors(task_instance_id="subject_relocations")
        .partial(
            observations=subject_group_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__subject_subtype",
                "extra__subject__sex",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_relocations") or {}),
        )
        .call()
    )

    subject_trajectory = (
        relocations_to_trajectory.validate()
        .handle_errors(task_instance_id="subject_trajectory")
        .partial(
            relocations=subject_relocations,
            **(params_dict.get("subject_trajectory") or {}),
        )
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .handle_errors(task_instance_id="traj_add_temporal_index")
        .partial(
            df=subject_trajectory,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    classify_trajectory_speed = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_trajectory_speed")
        .partial(
            df=traj_add_temporal_index,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_trajectory_speed") or {}),
        )
        .call()
    )

    split_subject_traj_groups = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_subject_traj_groups")
        .partial(
            df=classify_trajectory_speed,
            groupers=groupers,
            **(params_dict.get("split_subject_traj_groups") or {}),
        )
        .call()
    )

    sort_trajectories_speed = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_trajectories_speed")
        .partial(
            column_name="speed_bins",
            ascending=True,
            na_position="last",
            **(params_dict.get("sort_trajectories_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    colormap_traj_speed = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="colormap_traj_speed")
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("colormap_traj_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajectories_speed)
    )

    speed_bin_legend_with_unit = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="speed_bin_legend_with_unit")
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("speed_bin_legend_with_unit") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=colormap_traj_speed)
    )

    speed_val_with_unit = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="speed_val_with_unit")
        .partial(
            input_column_name="speed_kmhr",
            output_column_name="speed_kmhr",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("speed_val_with_unit") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=speed_bin_legend_with_unit)
    )

    rename_speed_display_columns = (
        map_columns.validate()
        .handle_errors(task_instance_id="rename_speed_display_columns")
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "segment_start": "Start",
                "timespan_seconds": "Duration (s)",
                "speed_kmhr": "Speed (kph)",
                "extra__name": "Subject Name",
                "extra__sex": "Subject Sex",
            },
            **(params_dict.get("rename_speed_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=speed_val_with_unit)
    )

    traj_map_layers = (
        create_polyline_layer.validate()
        .handle_errors(task_instance_id="traj_map_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "speed_bins_colormap"},
            legend={
                "label_column": "speed_bins_formatted",
                "color_column": "speed_bins_colormap",
            },
            tooltip_columns=[
                "Start",
                "Duration (s)",
                "Speed (kph)",
                "Nighttime",
                "Subject Name",
                "Subject Sex",
            ],
            **(params_dict.get("traj_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=rename_speed_display_columns)
    )

    summary_trajectory_table = (
        summarize_df.validate()
        .handle_errors(task_instance_id="summary_trajectory_table")
        .partial(
            groupby_cols=["extra__name"],
            summary_params=[
                {
                    "display_name": "Total Distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                    "decimal_places": 2,
                }
            ],
            **(params_dict.get("summary_trajectory_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
    )

    save_summary_table = (
        persist_df.validate()
        .handle_errors(task_instance_id="save_summary_table")
        .partial(
            filetype="csv",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("save_summary_table") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summary_trajectory_table)
    )

    combine_custom_map_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_custom_map_layers")
        .partial(
            static_layers=create_custom_map_layers,
            **(params_dict.get("combine_custom_map_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=traj_map_layers)
    )

    zoom_view_state = (
        create_view_state_from_gdf.validate()
        .handle_errors(task_instance_id="zoom_view_state")
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_view_state") or {}))
        .mapvalues(argnames=["gdf"], argvalues=rename_speed_display_columns)
    )

    zip_layers_view = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="zip_layers_view")
        .partial(
            left=combine_custom_map_layers,
            right=zoom_view_state,
            **(params_dict.get("zip_layers_view") or {}),
        )
        .call()
    )

    draw_combined_ecomap = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_combined_ecomap")
        .partial(
            tile_layers=base_map_defs,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Speeds(Km/h)"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_combined_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zip_layers_view)
    )

    ecomap_url = (
        persist_text.validate()
        .handle_errors(task_instance_id="ecomap_url")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("ecomap_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_combined_ecomap)
    )

    speed_map_widget = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="speed_map_widget")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(title="Speed Map", **(params_dict.get("speed_map_widget") or {}))
        .map(argnames=["view", "data"], argvalues=ecomap_url)
    )

    td_grouped_map_widget = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="td_grouped_map_widget")
        .partial(
            widgets=speed_map_widget, **(params_dict.get("td_grouped_map_widget") or {})
        )
        .call()
    )

    lg_dashboard = (
        gather_dashboard.validate()
        .handle_errors(task_instance_id="lg_dashboard")
        .partial(
            details=workflow_details,
            widgets=td_grouped_map_widget,
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("lg_dashboard") or {}),
        )
        .call()
    )

    return lg_dashboard

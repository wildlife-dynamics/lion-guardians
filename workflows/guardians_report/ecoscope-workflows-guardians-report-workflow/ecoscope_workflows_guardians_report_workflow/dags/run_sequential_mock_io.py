# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    set_patrols_and_patrol_events_params as set_patrols_and_patrol_events_params,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_styled_layers_from_gdf as create_styled_layers_from_gdf,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    download_file_and_persist as download_file_and_persist,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    make_text_layer as make_text_layer,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    set_custom_base_maps as set_custom_base_maps,
)

get_patrols_from_combined_params = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_patrols_from_combined_params",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_max as dataframe_column_max,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_mean as dataframe_column_mean,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.groupby import groupbykey as groupbykey
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    all_keyed_iterables_are_skips as all_keyed_iterables_are_skips,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_column_values_to_string as convert_column_values_to_string,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_core.tasks.transformation import with_unit as with_unit
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_scatterplot_layer as create_scatterplot_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_linear_time_density as calculate_linear_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    create_meshgrid as create_meshgrid,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    unpack_events_from_patrols_df_and_combined_params as unpack_events_from_patrols_df_and_combined_params,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_pie_chart as draw_pie_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_table as draw_table
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_time_series_bar_chart as draw_time_series_bar_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.skip import (
    all_geometry_are_none as all_geometry_are_none,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    drop_nan_values_by_column as drop_nan_values_by_column,
)
from ecoscope_workflows_ext_lion_guardians.tasks import add_totals_row as add_totals_row
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_cover_context_page as create_cover_context_page,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_report_context as create_report_context,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    draw_custom_map as draw_custom_map,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    extract_date_parts as extract_date_parts,
)
from ecoscope_workflows_ext_lion_guardians.tasks import flatten_tuple as flatten_tuple
from ecoscope_workflows_ext_lion_guardians.tasks import (
    get_event_type_display_names_from_events_aliased as get_event_type_display_names_from_events_aliased,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    get_patrol_observations_from_patrols_dataframe_and_combined_params as get_patrol_observations_from_patrols_dataframe_and_combined_params,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    get_split_group_names as get_split_group_names,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    merge_docx_files as merge_docx_files,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    merge_static_and_grouped_layers as merge_static_and_grouped_layers,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    view_state_deck_gdf as view_state_deck_gdf,
)
from ecoscope_workflows_ext_lion_guardians.tasks import zip_lists as zip_lists

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            timezone={
                "label": "UTC",
                "tzCode": "UTC",
                "name": "UTC",
                "utc_offset": "+00:00",
            },
            **(params_dict.get("time_range") or {}),
        )
        .call()
    )

    get_timezone = (
        get_timezone_from_time_range.validate()
        .set_task_instance_id("get_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(time_range=time_range, **(params_dict.get("get_timezone") or {}))
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    base_map_defs = (
        set_custom_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    persist_ambo_gpkg = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_ambo_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/phlc488gxqpcvr6ua3vk7/amboseli_group_ranch_boundaries.gpkg?rlkey=p5ztypwmj4ndjova9xe2ssiun&st=pknuicus&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_ambo_gpkg") or {}),
        )
        .call()
    )

    persist_cover_page = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/p1xk9no77w9ctpc4mnn6p/patrol_guardians_cover_page.docx?rlkey=tc5oo54s29wu7cz47glvlbcaj&st=9ivtf2k6&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_cover_page") or {}),
        )
        .call()
    )

    persist_indv_subject_page = (
        download_file_and_persist.validate()
        .set_task_instance_id("persist_indv_subject_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/kp9mkc9dd5qbast86ufk4/custom_patrol_template.docx?rlkey=ea12bxesuu9dnnj1ngfahhqvr&st=ri5og7k8&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            retries=3,
            unzip=False,
            **(params_dict.get("persist_indv_subject_page") or {}),
        )
        .call()
    )

    load_local_shapefiles = (
        load_df.validate()
        .set_task_instance_id("load_local_shapefiles")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=persist_ambo_gpkg,
            layer=None,
            deserialize_json=False,
            **(params_dict.get("load_local_shapefiles") or {}),
        )
        .call()
    )

    create_custom_map_layers = (
        create_styled_layers_from_gdf.validate()
        .set_task_instance_id("create_custom_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=load_local_shapefiles,
            filename="amboseli_group_ranch_boundaries",
            style_config={
                "styles": {
                    "stroked": True,
                    "filled": False,
                    "get_elevation": 50,
                    "opacity": 0.75,
                    "get_line_color": [105, 105, 105, 200],
                    "get_line_width": 3.75,
                },
                "legend": {"label": ["Group ranch boundaries"], "color": ["#696969"]},
            },
            **(params_dict.get("create_custom_map_layers") or {}),
        )
        .call()
    )

    custom_text_layer = (
        make_text_layer.validate()
        .set_task_instance_id("custom_text_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            txt_gdf=load_local_shapefiles,
            label_column="R_NAME",
            fallback_columns=["name", "title"],
            use_centroid=True,
            color=[0, 0, 0, 255],
            size=65,
            font_family="Calibri",
            font_weight="bold",
            background=False,
            background_color=None,
            background_padding=None,
            text_anchor="start",
            alignment_baseline="bottom",
            billboard=True,
            pickable=True,
            tooltip_columns=["label"],
            target_crs="epsg:4326",
            **(params_dict.get("custom_text_layer") or {}),
        )
        .call()
    )

    er_patrol_and_events_params = (
        set_patrols_and_patrol_events_params.validate()
        .set_task_instance_id("er_patrol_and_events_params")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            include_patrol_details=True,
            raise_on_empty=False,
            truncate_to_time_range=True,
            sub_page_size=150,
            **(params_dict.get("er_patrol_and_events_params") or {}),
        )
        .call()
    )

    prefetch_patrols = (
        get_patrols_from_combined_params.validate()
        .set_task_instance_id("prefetch_patrols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            combined_params=er_patrol_and_events_params,
            **(params_dict.get("prefetch_patrols") or {}),
        )
        .call()
    )

    patrol_obs = (
        get_patrol_observations_from_patrols_dataframe_and_combined_params.validate()
        .set_task_instance_id("patrol_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            patrols_df=prefetch_patrols,
            combined_params=er_patrol_and_events_params,
            **(params_dict.get("patrol_obs") or {}),
        )
        .call()
    )

    patrol_events = (
        unpack_events_from_patrols_df_and_combined_params.validate()
        .set_task_instance_id("patrol_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            patrols_df=prefetch_patrols,
            combined_params=er_patrol_and_events_params,
            **(params_dict.get("patrol_events") or {}),
        )
        .call()
    )

    event_type_display_names = (
        get_event_type_display_names_from_events_aliased.validate()
        .set_task_instance_id("event_type_display_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            events_gdf=patrol_events,
            append_category_names="duplicates",
            **(params_dict.get("event_type_display_names") or {}),
        )
        .call()
    )

    convert_patrols_to_user_timezone = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_patrols_to_user_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_obs,
            timezone=get_timezone,
            columns=["patrol_start_time", "patrol_end_time", "fixtime"],
            **(params_dict.get("convert_patrols_to_user_timezone") or {}),
        )
        .call()
    )

    convert_events_to_user_timezone = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_events_to_user_timezone")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=event_type_display_names,
            timezone=get_timezone,
            columns=["time", "patrol_start_time"],
            **(params_dict.get("convert_events_to_user_timezone") or {}),
        )
        .call()
    )

    persist_events_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_events_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            df=convert_events_to_user_timezone,
            filename="events",
            **(params_dict.get("persist_events_geoparquet") or {}),
        )
        .call()
    )

    set_patrol_traj_color_column = (
        set_string_var.validate()
        .set_task_instance_id("set_patrol_traj_color_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("set_patrol_traj_color_column") or {}))
        .call()
    )

    patrol_reloc = (
        process_relocations.validate()
        .set_task_instance_id("patrol_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=convert_patrols_to_user_timezone,
            relocs_columns=[
                "patrol_id",
                "patrol_start_time",
                "patrol_end_time",
                "patrol_type__value",
                "patrol_type__display",
                "patrol_serial_number",
                "patrol_status",
                "patrol_subject",
                "groupby_col",
                "fixtime",
                "junk_status",
                "extra__source",
                "geometry",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("patrol_reloc") or {}),
        )
        .call()
    )

    patrol_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("patrol_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(relocations=patrol_reloc, **(params_dict.get("patrol_traj") or {}))
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_traj,
            time_col="extra__patrol_start_time",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    traj_rename_grouper_columns = (
        map_columns.validate()
        .set_task_instance_id("traj_rename_grouper_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__patrol_type__value": "patrol_type",
                "extra__patrol_serial_number": "patrol_serial_number",
                "extra__patrol_status": "patrol_status",
                "extra__patrol_subject": "patrol_subject",
            },
            **(params_dict.get("traj_rename_grouper_columns") or {}),
        )
        .call()
    )

    persist_patrols_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_patrols_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            df=traj_rename_grouper_columns,
            filename="trajectories",
            **(params_dict.get("persist_patrols_geoparquet") or {}),
        )
        .call()
    )

    traj_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("traj_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_rename_grouper_columns,
            colormap="Paired",
            input_column_name=set_patrol_traj_color_column,
            output_column_name="patrol_traj_colormap",
            **(params_dict.get("traj_colormap") or {}),
        )
        .call()
    )

    filter_patrol_events = (
        apply_reloc_coord_filter.validate()
        .set_task_instance_id("filter_patrol_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_events_to_user_timezone,
            roi_gdf=None,
            roi_name=None,
            **(params_dict.get("filter_patrol_events") or {}),
        )
        .call()
    )

    pe_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("pe_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_patrol_events,
            time_col="patrol_start_time",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("pe_add_temporal_index") or {}),
        )
        .call()
    )

    pe_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("pe_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_add_temporal_index,
            input_column_name="event_type",
            colormap="Accent",
            output_column_name="event_type_colormap",
            **(params_dict.get("pe_colormap") or {}),
        )
        .call()
    )

    patrol_traj_cols_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("patrol_traj_cols_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_colormap,
            columns=["patrol_serial_number", "patrol_type"],
            **(params_dict.get("patrol_traj_cols_to_string") or {}),
        )
        .call()
    )

    pe_cols_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("pe_cols_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_colormap,
            columns=["patrol_serial_number", "patrol_type"],
            **(params_dict.get("pe_cols_to_string") or {}),
        )
        .call()
    )

    set_traj_pe_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_traj_pe_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Trajectories & Patrol Events Map",
            **(params_dict.get("set_traj_pe_map_title") or {}),
        )
        .call()
    )

    set_ltd_map_title = (
        set_string_var.validate()
        .set_task_instance_id("set_ltd_map_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(var="Time Density Map", **(params_dict.get("set_ltd_map_title") or {}))
        .call()
    )

    set_bar_chart_title = (
        set_string_var.validate()
        .set_task_instance_id("set_bar_chart_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Patrol Events Bar Chart",
            **(params_dict.get("set_bar_chart_title") or {}),
        )
        .call()
    )

    set_pie_chart_title = (
        set_string_var.validate()
        .set_task_instance_id("set_pie_chart_title")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="Patrol Events Pie Chart",
            **(params_dict.get("set_pie_chart_title") or {}),
        )
        .call()
    )

    split_patrol_traj_groups = (
        split_groups.validate()
        .set_task_instance_id("split_patrol_traj_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_traj_cols_to_string,
            groupers=groupers,
            **(params_dict.get("split_patrol_traj_groups") or {}),
        )
        .call()
    )

    split_pe_groups = (
        split_groups.validate()
        .set_task_instance_id("split_pe_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_cols_to_string,
            groupers=groupers,
            **(params_dict.get("split_pe_groups") or {}),
        )
        .call()
    )

    pe_rename_display_columns = (
        map_columns.validate()
        .set_task_instance_id("pe_rename_display_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "patrol_serial_number": "Patrol Serial",
                "serial_number": "Event Serial",
                "event_type_display": "Event Type",
                "time": "Event Time",
            },
            **(params_dict.get("pe_rename_display_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_pe_groups)
    )

    patrol_events_map_layers = (
        create_scatterplot_layer.validate()
        .set_task_instance_id("patrol_events_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": "event_type_colormap",
                "get_radius": 5,
                "opacity": 0.55,
                "stroked": True,
            },
            legend=None,
            **(params_dict.get("patrol_events_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=pe_rename_display_columns)
    )

    speed_val_with_unit = (
        map_values_with_unit.validate()
        .set_task_instance_id("speed_val_with_unit")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_kmhr",
            output_column_name="speed_kmhr",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("speed_val_with_unit") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    patrol_traj_rename_columns = (
        map_columns.validate()
        .set_task_instance_id("patrol_traj_rename_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "patrol_serial_number": "Patrol Serial",
                "extra__patrol_type__display": "Patrol Type",
                "segment_start": "Start",
                "timespan_seconds": "Duration (s)",
                "speed_kmhr": "Speed (kph)",
            },
            **(params_dict.get("patrol_traj_rename_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=speed_val_with_unit)
    )

    patrol_traj_map_layers = (
        create_path_layer.validate()
        .set_task_instance_id("patrol_traj_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "patrol_traj_colormap",
                "get_width": 1.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 6,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "label_column": set_patrol_traj_color_column,
                "color_column": "patrol_traj_colormap",
            },
            **(params_dict.get("patrol_traj_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=patrol_traj_rename_columns)
    )

    combined_traj_and_pe_map_layers = (
        groupbykey.validate()
        .set_task_instance_id("combined_traj_and_pe_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                all_keyed_iterables_are_skips,
            ],
            unpack_depth=1,
        )
        .partial(
            iterables=[patrol_traj_map_layers, patrol_events_map_layers],
            **(params_dict.get("combined_traj_and_pe_map_layers") or {}),
        )
        .call()
    )

    merge_static_grouped_layers = (
        merge_static_and_grouped_layers.validate()
        .set_task_instance_id("merge_static_grouped_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_custom_map_layers, custom_text_layer],
            **(params_dict.get("merge_static_grouped_layers") or {}),
        )
        .mapvalues(
            argnames=["grouped_layers"], argvalues=combined_traj_and_pe_map_layers
        )
    )

    zoom_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("zoom_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            pitch=0,
            bearing=0,
            gdf=load_local_shapefiles,
            **(params_dict.get("zoom_view_state") or {}),
        )
        .call()
    )

    traj_patrol_events_ecomap = (
        draw_custom_map.validate()
        .set_task_instance_id("traj_patrol_events_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            legend_style={
                "title": set_patrol_traj_color_column,
                "placement": "bottom-right",
            },
            static=False,
            title=None,
            max_zoom=15,
            widget_id=set_traj_pe_map_title,
            view_state=zoom_view_state,
            **(params_dict.get("traj_patrol_events_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=merge_static_grouped_layers)
    )

    traj_pe_ecomap_html_urls = (
        persist_text.validate()
        .set_task_instance_id("traj_pe_ecomap_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="patrols_ecomap",
            **(params_dict.get("traj_pe_ecomap_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=traj_patrol_events_ecomap)
    )

    traj_pe_map_widgets_single_views = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("traj_pe_map_widgets_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_traj_pe_map_title,
            **(params_dict.get("traj_pe_map_widgets_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=traj_pe_ecomap_html_urls)
    )

    traj_pe_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("traj_pe_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=traj_pe_map_widgets_single_views,
            **(params_dict.get("traj_pe_grouped_map_widget") or {}),
        )
        .call()
    )

    patrol_html_png = (
        html_to_png.validate()
        .set_task_instance_id("patrol_html_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 20000},
            **(params_dict.get("patrol_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=traj_pe_ecomap_html_urls)
    )

    total_patrols = (
        dataframe_column_nunique.validate()
        .set_task_instance_id("total_patrols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="extra__patrol_id", **(params_dict.get("total_patrols") or {})
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    total_patrols_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_patrols_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Patrols",
            decimal_places=1,
            **(params_dict.get("total_patrols_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_patrols)
    )

    total_patrols_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_patrols_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_patrols_sv_widgets,
            **(params_dict.get("total_patrols_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_patrol_time = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_patrol_time")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="timespan_seconds",
            **(params_dict.get("total_patrol_time") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    total_patrol_time_converted = (
        with_unit.validate()
        .set_task_instance_id("total_patrol_time_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="s",
            new_unit="h",
            **(params_dict.get("total_patrol_time_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=total_patrol_time)
    )

    total_patrol_time_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_patrol_time_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Time",
            decimal_places=1,
            **(params_dict.get("total_patrol_time_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_patrol_time_converted)
    )

    patrol_time_grouped_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("patrol_time_grouped_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_patrol_time_sv_widgets,
            **(params_dict.get("patrol_time_grouped_widget") or {}),
        )
        .call()
    )

    total_patrol_dist = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_patrol_dist")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="dist_meters", **(params_dict.get("total_patrol_dist") or {})
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    total_patrol_dist_converted = (
        with_unit.validate()
        .set_task_instance_id("total_patrol_dist_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="m",
            new_unit="km",
            **(params_dict.get("total_patrol_dist_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=total_patrol_dist)
    )

    total_patrol_dist_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_patrol_dist_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Distance",
            decimal_places=1,
            **(params_dict.get("total_patrol_dist_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=total_patrol_dist_converted)
    )

    patrol_dist_grouped_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("patrol_dist_grouped_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_patrol_dist_sv_widgets,
            **(params_dict.get("patrol_dist_grouped_widget") or {}),
        )
        .call()
    )

    avg_speed = (
        dataframe_column_mean.validate()
        .set_task_instance_id("avg_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="speed_kmhr", **(params_dict.get("avg_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    average_speed_converted = (
        with_unit.validate()
        .set_task_instance_id("average_speed_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="km/h",
            new_unit="km/h",
            **(params_dict.get("average_speed_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=avg_speed)
    )

    avg_speed_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("avg_speed_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Average Speed",
            decimal_places=1,
            **(params_dict.get("avg_speed_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=average_speed_converted)
    )

    avg_speed_grouped_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("avg_speed_grouped_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=avg_speed_sv_widgets,
            **(params_dict.get("avg_speed_grouped_widget") or {}),
        )
        .call()
    )

    max_speed = (
        dataframe_column_max.validate()
        .set_task_instance_id("max_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="speed_kmhr", **(params_dict.get("max_speed") or {}))
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    max_speed_converted = (
        with_unit.validate()
        .set_task_instance_id("max_speed_converted")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            original_unit="km/h",
            new_unit="km/h",
            **(params_dict.get("max_speed_converted") or {}),
        )
        .mapvalues(argnames=["value"], argvalues=max_speed)
    )

    max_speed_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("max_speed_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Max Speed",
            decimal_places=1,
            **(params_dict.get("max_speed_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=max_speed_converted)
    )

    max_speed_grouped_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("max_speed_grouped_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=max_speed_sv_widgets,
            **(params_dict.get("max_speed_grouped_widget") or {}),
        )
        .call()
    )

    patrol_events_bar_chart = (
        draw_time_series_bar_chart.validate()
        .set_task_instance_id("patrol_events_bar_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            x_axis="time",
            y_axis="event_type_display",
            category="event_type_display",
            agg_function="count",
            color_column="event_type_colormap",
            plot_style={"xperiodalignment": "middle"},
            layout_style=None,
            widget_id=set_bar_chart_title,
            **(params_dict.get("patrol_events_bar_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=split_pe_groups)
    )

    patrol_events_bar_chart_html_url = (
        persist_text.validate()
        .set_task_instance_id("patrol_events_bar_chart_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="patrol_events_time_series_bar_chart",
            **(params_dict.get("patrol_events_bar_chart_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=patrol_events_bar_chart)
    )

    patrol_events_bar_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("patrol_events_bar_chart_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_bar_chart_title,
            **(params_dict.get("patrol_events_bar_chart_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=patrol_events_bar_chart_html_url)
    )

    grouped_bar_plot_widget_merge = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_bar_plot_widget_merge")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=patrol_events_bar_chart_widget,
            **(params_dict.get("grouped_bar_plot_widget_merge") or {}),
        )
        .call()
    )

    patrol_bar_chart_png = (
        html_to_png.validate()
        .set_task_instance_id("patrol_bar_chart_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 1000},
            **(params_dict.get("patrol_bar_chart_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=patrol_events_bar_chart_html_url)
    )

    patrol_events_pie_chart = (
        draw_pie_chart.validate()
        .set_task_instance_id("patrol_events_pie_chart")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            value_column="event_type_display",
            plot_style={"textinfo": "value"},
            label_column=None,
            color_column="event_type_colormap",
            layout_style=None,
            widget_id=set_pie_chart_title,
            **(params_dict.get("patrol_events_pie_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=split_pe_groups)
    )

    pe_pie_chart_html_urls = (
        persist_text.validate()
        .set_task_instance_id("pe_pie_chart_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="patrols_pie_chart",
            **(params_dict.get("pe_pie_chart_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=patrol_events_pie_chart)
    )

    patrol_events_pie_chart_widgets = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("patrol_events_pie_chart_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title=set_pie_chart_title,
            **(params_dict.get("patrol_events_pie_chart_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=pe_pie_chart_html_urls)
    )

    patrol_events_pie_widget_grouped = (
        merge_widget_views.validate()
        .set_task_instance_id("patrol_events_pie_widget_grouped")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=patrol_events_pie_chart_widgets,
            **(params_dict.get("patrol_events_pie_widget_grouped") or {}),
        )
        .call()
    )

    patrol_pie_chart_png = (
        html_to_png.validate()
        .set_task_instance_id("patrol_pie_chart_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 1000},
            **(params_dict.get("patrol_pie_chart_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=pe_pie_chart_html_urls)
    )

    ltd_meshgrid = (
        create_meshgrid.validate()
        .set_task_instance_id("ltd_meshgrid")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            aoi=patrol_traj_cols_to_string,
            intersecting_only=False,
            **(params_dict.get("ltd_meshgrid") or {}),
        )
        .call()
    )

    ltd = (
        calculate_linear_time_density.validate()
        .set_task_instance_id("ltd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            meshgrid=ltd_meshgrid,
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 100.0],
            **(params_dict.get("ltd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_patrol_traj_groups)
    )

    drop_nan_percentiles = (
        drop_nan_values_by_column.validate()
        .set_task_instance_id("drop_nan_percentiles")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="percentile", **(params_dict.get("drop_nan_percentiles") or {})
        )
        .mapvalues(argnames=["df"], argvalues=ltd)
    )

    sort_percentile_values = (
        sort_values.validate()
        .set_task_instance_id("sort_percentile_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="percentile",
            ascending=True,
            na_position="last",
            **(params_dict.get("sort_percentile_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_nan_percentiles)
    )

    percentile_col_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("percentile_col_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["percentile"],
            **(params_dict.get("percentile_col_to_string") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_percentile_values)
    )

    td_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("td_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=percentile_col_to_string)
    )

    patrol_td_rename_columns = (
        map_columns.validate()
        .set_task_instance_id("patrol_td_rename_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={"percentile": "Percentile"},
            **(params_dict.get("patrol_td_rename_columns") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=td_colormap)
    )

    td_map_layer = (
        create_geojson_layer.validate()
        .set_task_instance_id("td_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": "percentile_colormap",
                "opacity": 0.45,
                "get_line_width": 0.75,
                "stroked": True,
            },
            legend={
                "label_column": "Percentile",
                "color_column": "percentile_colormap",
            },
            **(params_dict.get("td_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=patrol_td_rename_columns)
    )

    merged_time_density_layers = (
        merge_static_and_grouped_layers.validate()
        .set_task_instance_id("merged_time_density_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_custom_map_layers, custom_text_layer],
            **(params_dict.get("merged_time_density_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=td_map_layer)
    )

    td_ecomap = (
        draw_custom_map.validate()
        .set_task_instance_id("td_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            legend_style={"title": "Time Spent", "placement": "bottom-right"},
            static=False,
            title=None,
            max_zoom=15,
            widget_id=set_ltd_map_title,
            view_state=zoom_view_state,
            **(params_dict.get("td_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=merged_time_density_layers)
    )

    td_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("td_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="time_density",
            **(params_dict.get("td_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=td_ecomap)
    )

    td_map_widget = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("td_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(title=set_ltd_map_title, **(params_dict.get("td_map_widget") or {}))
        .map(argnames=["view", "data"], argvalues=td_ecomap_html_url)
    )

    td_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("td_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=td_map_widget, **(params_dict.get("td_grouped_map_widget") or {})
        )
        .call()
    )

    td_html_png = (
        html_to_png.validate()
        .set_task_instance_id("td_html_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 20000},
            **(params_dict.get("td_html_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=td_ecomap_html_url)
    )

    summarize_ranger_patrol = (
        summarize_df.validate()
        .set_task_instance_id("summarize_ranger_patrol")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["patrol_subject"],
            reset_index=True,
            summary_params=[
                {
                    "display_name": "no_of_patrols",
                    "aggregator": "nunique",
                    "column": "extra__patrol_id",
                },
                {
                    "display_name": "total_distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                },
                {
                    "display_name": "total_time",
                    "aggregator": "sum",
                    "column": "timespan_seconds",
                    "original_unit": "s",
                    "new_unit": "h",
                },
            ],
            **(params_dict.get("summarize_ranger_patrol") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    persist_ranger_patrol_efforts = (
        persist_df.validate()
        .set_task_instance_id("persist_ranger_patrol_efforts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_ranger_patrol_efforts") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summarize_ranger_patrol)
    )

    summarized_patrol_types = (
        summarize_df.validate()
        .set_task_instance_id("summarized_patrol_types")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["patrol_type"],
            reset_index=True,
            summary_params=[
                {
                    "display_name": "no_of_patrols",
                    "aggregator": "nunique",
                    "column": "extra__patrol_id",
                },
                {
                    "display_name": "total_distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                },
                {
                    "display_name": "total_time",
                    "aggregator": "sum",
                    "column": "timespan_seconds",
                    "original_unit": "s",
                    "new_unit": "h",
                },
            ],
            **(params_dict.get("summarized_patrol_types") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    add_total_patrol_summary = (
        add_totals_row.validate()
        .set_task_instance_id("add_total_patrol_summary")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            label_col="patrol_type",
            label="Total",
            **(params_dict.get("add_total_patrol_summary") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summarized_patrol_types)
    )

    persist_patrol_types = (
        persist_df.validate()
        .set_task_instance_id("persist_patrol_types")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_patrol_types") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=add_total_patrol_summary)
    )

    summarize_guardian_events = (
        summarize_df.validate()
        .set_task_instance_id("summarize_guardian_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["patrol_subject"],
            reset_index=True,
            summary_params=[
                {
                    "display_name": "no_of_events",
                    "aggregator": "nunique",
                    "column": "id",
                }
            ],
            **(params_dict.get("summarize_guardian_events") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=pe_rename_display_columns)
    )

    persist_gua_patrol_efforts = (
        persist_df.validate()
        .set_task_instance_id("persist_gua_patrol_efforts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_gua_patrol_efforts") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summarize_guardian_events)
    )

    summarized_event_types = (
        summarize_df.validate()
        .set_task_instance_id("summarized_event_types")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["event_type"],
            reset_index=True,
            summary_params=[
                {
                    "display_name": "no_of_events",
                    "aggregator": "nunique",
                    "column": "id",
                }
            ],
            **(params_dict.get("summarized_event_types") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=pe_rename_display_columns)
    )

    persist_event_tefforts = (
        persist_df.validate()
        .set_task_instance_id("persist_event_tefforts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_event_tefforts") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summarized_event_types)
    )

    add_month_name = (
        extract_date_parts.validate()
        .set_task_instance_id("add_month_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            date_column="extra__patrol_start_time",
            parts=["month", "day", "month_name"],
            **(params_dict.get("add_month_name") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_patrol_traj_groups)
    )

    summarize_month_patrol = (
        summarize_df.validate()
        .set_task_instance_id("summarize_month_patrol")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["month_name"],
            reset_index=True,
            summary_params=[
                {
                    "display_name": "no_of_patrols",
                    "aggregator": "nunique",
                    "column": "extra__patrol_id",
                },
                {
                    "display_name": "total_distance",
                    "aggregator": "sum",
                    "column": "dist_meters",
                    "original_unit": "m",
                    "new_unit": "km",
                },
                {
                    "display_name": "total_time",
                    "aggregator": "sum",
                    "column": "timespan_seconds",
                    "original_unit": "s",
                    "new_unit": "h",
                },
            ],
            **(params_dict.get("summarize_month_patrol") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=add_month_name)
    )

    persist_month_patrol_efforts = (
        persist_df.validate()
        .set_task_instance_id("persist_month_patrol_efforts")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            **(params_dict.get("persist_month_patrol_efforts") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=summarize_month_patrol)
    )

    no_of_events_recorded = (
        draw_table.validate()
        .set_task_instance_id("no_of_events_recorded")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widget_id="No of events recorded by guardian",
            **(params_dict.get("no_of_events_recorded") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=summarize_guardian_events)
    )

    no_of_events_recorded_url = (
        persist_text.validate()
        .set_task_instance_id("no_of_events_recorded_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="no_of_events_recorded_table",
            **(params_dict.get("no_of_events_recorded_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=no_of_events_recorded)
    )

    no_events_recorded_sv = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("no_events_recorded_sv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="No of events recorded by Guardian",
            **(params_dict.get("no_events_recorded_sv") or {}),
        )
        .map(argnames=["view", "data"], argvalues=no_of_events_recorded_url)
    )

    no_of_events_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("no_of_events_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=no_events_recorded_sv,
            **(params_dict.get("no_of_events_table_widget") or {}),
        )
        .call()
    )

    ranger_patrol_recorded = (
        draw_table.validate()
        .set_task_instance_id("ranger_patrol_recorded")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widget_id="Ranger patrols summary",
            **(params_dict.get("ranger_patrol_recorded") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=summarize_ranger_patrol)
    )

    ranger_patrol_recorded_url = (
        persist_text.validate()
        .set_task_instance_id("ranger_patrol_recorded_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="ranger_patrol_recorded_table",
            **(params_dict.get("ranger_patrol_recorded_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=ranger_patrol_recorded)
    )

    ranger_patrols_sv = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("ranger_patrols_sv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Ranger patrol summary by Guardian",
            **(params_dict.get("ranger_patrols_sv") or {}),
        )
        .map(argnames=["view", "data"], argvalues=ranger_patrol_recorded_url)
    )

    ranger_patrol_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("ranger_patrol_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=ranger_patrols_sv,
            **(params_dict.get("ranger_patrol_table_widget") or {}),
        )
        .call()
    )

    patrol_types_recorded = (
        draw_table.validate()
        .set_task_instance_id("patrol_types_recorded")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widget_id="Patrol types summary",
            **(params_dict.get("patrol_types_recorded") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=summarized_patrol_types)
    )

    patrol_types_recorded_url = (
        persist_text.validate()
        .set_task_instance_id("patrol_types_recorded_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="patrol_types_recorded_table",
            **(params_dict.get("patrol_types_recorded_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=patrol_types_recorded)
    )

    patrol_types_sv = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("patrol_types_sv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Patrol types summary", **(params_dict.get("patrol_types_sv") or {})
        )
        .map(argnames=["view", "data"], argvalues=patrol_types_recorded_url)
    )

    patrol_types_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("patrol_types_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=patrol_types_sv,
            **(params_dict.get("patrol_types_table_widget") or {}),
        )
        .call()
    )

    event_types_recorded = (
        draw_table.validate()
        .set_task_instance_id("event_types_recorded")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widget_id="Event types summary",
            **(params_dict.get("event_types_recorded") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=summarized_event_types)
    )

    event_types_recorded_url = (
        persist_text.validate()
        .set_task_instance_id("event_types_recorded_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="event_types_recorded_table",
            **(params_dict.get("event_types_recorded_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=event_types_recorded)
    )

    event_types_sv = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("event_types_sv")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Event types summary", **(params_dict.get("event_types_sv") or {})
        )
        .map(argnames=["view", "data"], argvalues=event_types_recorded_url)
    )

    event_types_table_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("event_types_table_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=event_types_sv,
            **(params_dict.get("event_types_table_widget") or {}),
        )
        .call()
    )

    context_cover_page = (
        create_cover_context_page.validate()
        .set_task_instance_id("context_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            report_period=time_range,
            prepared_by="Ecoscope",
            template_path=persist_cover_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="cover_page.docx",
            **(params_dict.get("context_cover_page") or {}),
        )
        .call()
    )

    report_context = (
        groupbykey.validate()
        .set_task_instance_id("report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                all_keyed_iterables_are_skips,
            ],
            unpack_depth=1,
        )
        .partial(
            iterables=[
                persist_patrol_types,
                patrol_html_png,
                td_html_png,
                patrol_pie_chart_png,
                patrol_bar_chart_png,
                persist_gua_patrol_efforts,
                persist_event_tefforts,
                persist_month_patrol_efforts,
                persist_ranger_patrol_efforts,
            ],
            **(params_dict.get("report_context") or {}),
        )
        .call()
    )

    flatten_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=report_context)
    )

    get_grouper_names = (
        get_split_group_names.validate()
        .set_task_instance_id("get_grouper_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            split_data=split_patrol_traj_groups,
            **(params_dict.get("get_grouper_names") or {}),
        )
        .call()
    )

    zip_grouper_with_context = (
        zip_lists.validate()
        .set_task_instance_id("zip_grouper_with_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=get_grouper_names,
            right=flatten_context,
            **(params_dict.get("zip_grouper_with_context") or {}),
        )
        .call()
    )

    flatten_final_report_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_final_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_final_report_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=zip_grouper_with_context)
    )

    individual_report_context = (
        create_report_context.validate()
        .set_task_instance_id("individual_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filename=None,
            validate_images=True,
            box_h_cm=9.0,
            box_w_cm=15.0,
            template_path=persist_indv_subject_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("individual_report_context") or {}),
        )
        .mapvalues(
            argnames=[
                "grouper_type",
                "grouper_eq",
                "grouper_value",
                "patrol_type_effort_path",
                "patrol_events_track_map",
                "patrol_time_density_map",
                "events_pie_chart",
                "events_time_series_bar_chart",
                "patrol_events",
                "event_efforts",
                "month_stats",
                "guardian_stats",
            ],
            argvalues=flatten_final_report_context,
        )
    )

    generate_report = (
        merge_docx_files.validate()
        .set_task_instance_id("generate_report")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=context_cover_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=individual_report_context,
            filename="lg_guardians_report.docx",
            **(params_dict.get("generate_report") or {}),
        )
        .call()
    )

    patrol_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("patrol_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[
                traj_pe_grouped_map_widget,
                td_grouped_map_widget,
                grouped_bar_plot_widget_merge,
                patrol_events_pie_widget_grouped,
                total_patrols_grouped_sv_widget,
                patrol_time_grouped_widget,
                patrol_dist_grouped_widget,
                avg_speed_grouped_widget,
                max_speed_grouped_widget,
                no_of_events_table_widget,
                ranger_patrol_table_widget,
                patrol_types_table_widget,
                event_types_table_widget,
            ],
            groupers=groupers,
            time_range=time_range,
            **(params_dict.get("patrol_dashboard") or {}),
        )
        .call()
    )

    return patrol_dashboard

# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


# ruff: noqa: E402

# %% [markdown]
# # Collared Lions
# TODO: top level description

# %% [markdown]
# ## Imports

import os

from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.spatial_ops import (
    reproject_gdf as reproject_gdf,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_cl_ctx_cover as create_cl_ctx_cover,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_collared_lions_grouper_ctx as create_collared_lions_grouper_ctx,
)
from ecoscope_workflows_ext_lion_guardians.tasks import (
    create_context_page_lg as create_context_page_lg,
)
from ecoscope_workflows_ext_lion_guardians.tasks import merge_cl_files as merge_cl_files
from ecoscope_workflows_ext_mnc.tasks import add_totals_row as add_totals_row
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_custom_text_layer as create_custom_text_layer,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layer_from_gdf as create_deckgl_layer_from_gdf,
)
from ecoscope_workflows_ext_ste.tasks import create_grouper_page as create_grouper_page
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import get_gdf_geom_type as get_gdf_geom_type
from ecoscope_workflows_ext_ste.tasks import round_off_values as round_off_values
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

# %% [markdown]
# ## Set Workflow Details

# %%
# parameters

workflow_details_params = dict(
    name=...,
    description=...,
    image_url=...,
)

# %%
# call the task


workflow_details = (
    set_workflow_details.set_task_instance_id("workflow_details")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**workflow_details_params)
    .call()
)


# %% [markdown]
# ## Define time range

# %%
# parameters

time_range_params = dict(
    since=...,
    until=...,
)

# %%
# call the task


time_range = (
    set_time_range.set_task_instance_id("time_range")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        time_format="%d %b %Y %H:%M:%S %Z",
        timezone={
            "label": "UTC",
            "tzCode": "UTC",
            "name": "UTC",
            "utc_offset": "+03:00",
        },
        **time_range_params,
    )
    .call()
)


# %% [markdown]
# ## Set Groupers

# %%
# parameters

groupers_params = dict(
    groupers=...,
)

# %%
# call the task


groupers = (
    set_groupers.set_task_instance_id("groupers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**groupers_params)
    .call()
)


# %% [markdown]
# ## Data Source

# %%
# parameters

er_client_name_params = dict(
    data_source=...,
)

# %%
# call the task


er_client_name = (
    set_er_connection.set_task_instance_id("er_client_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**er_client_name_params)
    .call()
)


# %% [markdown]
# ## Base Maps

# %%
# parameters

base_map_defs_params = dict(
    base_maps=...,
)

# %%
# call the task


base_map_defs = (
    set_base_maps_pydeck.set_task_instance_id("base_map_defs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**base_map_defs_params)
    .call()
)


# %% [markdown]
# ## Download amboseli group ranches

# %%
# parameters

persist_ambo_gpkg_params = dict()

# %%
# call the task


persist_ambo_gpkg = (
    fetch_and_persist_file.set_task_instance_id("persist_ambo_gpkg")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/fcy57d5x5y67gh0xje6rw/lg_group_ranch_boundaries.gpkg?rlkey=yte99xhkte7f1r7n9nemgy7p0&st=sxx1emc1&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        retries=3,
        unzip=False,
        **persist_ambo_gpkg_params,
    )
    .call()
)


# %% [markdown]
# ## Download hotspot areas gpkg

# %%
# parameters

persist_hotspot_areas_params = dict()

# %%
# call the task


persist_hotspot_areas = (
    fetch_and_persist_file.set_task_instance_id("persist_hotspot_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/nlozcti0oqhj6bzvsr46g/lg_conflict_hotspots.gpkg?rlkey=giwdizp1j6e24btgh48uxf5v0&st=jivblame&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        retries=3,
        unzip=False,
        **persist_hotspot_areas_params,
    )
    .call()
)


# %% [markdown]
# ## Download protected areas gpkg

# %%
# parameters

persist_protected_gpkg_params = dict()

# %%
# call the task


persist_protected_gpkg = (
    fetch_and_persist_file.set_task_instance_id("persist_protected_gpkg")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/i5yczgyln3zh1n8c4ppl5/lg_protected_areas.gpkg?rlkey=5ea21haq2tmsmx7g502p3qag5&st=zt6ztcku&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        retries=3,
        unzip=False,
        **persist_protected_gpkg_params,
    )
    .call()
)


# %% [markdown]
# ## Download collared lions cover page

# %%
# parameters

persist_cover_page_params = dict()

# %%
# call the task


persist_cover_page = (
    fetch_and_persist_file.set_task_instance_id("persist_cover_page")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/kyjd9ii9nul1osbkezl5w/collared_lions_cover_page.docx?rlkey=4nl68thyqzd0n49wnr1770u0x&st=fnpsvt9i&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        retries=3,
        unzip=False,
        **persist_cover_page_params,
    )
    .call()
)


# %% [markdown]
# ## Download collared subject page

# %%
# parameters

persist_indv_subject_page_params = dict()

# %%
# call the task


persist_indv_subject_page = (
    fetch_and_persist_file.set_task_instance_id("persist_indv_subject_page")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/v69q5ja7whtb033adq829/collared_lion_subject_template.docx?rlkey=zaf9iknc61zukii4xzwwwuu6o&st=1gj1vbk3&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        retries=3,
        unzip=False,
        **persist_indv_subject_page_params,
    )
    .call()
)


# %% [markdown]
# ## Load amboseli group ranches

# %%
# parameters

load_ambo_group_ranches_params = dict()

# %%
# call the task


load_ambo_group_ranches = (
    load_df.set_task_instance_id("load_ambo_group_ranches")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        file_path=persist_ambo_gpkg,
        layer=None,
        deserialize_json=False,
        **load_ambo_group_ranches_params,
    )
    .call()
)


# %% [markdown]
# ## Load amboseli group ranches

# %%
# parameters

load_hotspot_areas_params = dict()

# %%
# call the task


load_hotspot_areas = (
    load_df.set_task_instance_id("load_hotspot_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        file_path=persist_hotspot_areas,
        layer=None,
        deserialize_json=False,
        **load_hotspot_areas_params,
    )
    .call()
)


# %% [markdown]
# ## Load protected areas

# %%
# parameters

load_protected_areas_params = dict()

# %%
# call the task


load_protected_areas = (
    load_df.set_task_instance_id("load_protected_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        file_path=persist_protected_gpkg,
        layer=None,
        deserialize_json=False,
        **load_protected_areas_params,
    )
    .call()
)


# %% [markdown]
# ## Reproject gdf to 4326 for ambo layers

# %%
# parameters

reproject_ambo_boundaries_params = dict()

# %%
# call the task


reproject_ambo_boundaries = (
    reproject_gdf.set_task_instance_id("reproject_ambo_boundaries")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=load_ambo_group_ranches,
        target_crs="EPSG:4326",
        **reproject_ambo_boundaries_params,
    )
    .call()
)


# %% [markdown]
# ## Reproject gdf to 4326 for hotspot areas

# %%
# parameters

reproject_hotspot_areas_params = dict()

# %%
# call the task


reproject_hotspot_areas = (
    reproject_gdf.set_task_instance_id("reproject_hotspot_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=load_hotspot_areas, target_crs="EPSG:4326", **reproject_hotspot_areas_params
    )
    .call()
)


# %% [markdown]
# ## Reproject gdf to 4326 for protected areas

# %%
# parameters

reproject_protected_areas_params = dict()

# %%
# call the task


reproject_protected_areas = (
    reproject_gdf.set_task_instance_id("reproject_protected_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=load_protected_areas,
        target_crs="EPSG:4326",
        **reproject_protected_areas_params,
    )
    .call()
)


# %% [markdown]
# ## Annotate amboseli layers with geom type

# %%
# parameters

annotate_ambo_layers_params = dict()

# %%
# call the task


annotate_ambo_layers = (
    get_gdf_geom_type.set_task_instance_id("annotate_ambo_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=reproject_ambo_boundaries, **annotate_ambo_layers_params)
    .call()
)


# %% [markdown]
# ## Annotate hotspot layers with geom type

# %%
# parameters

annotate_hotspot_layers_params = dict()

# %%
# call the task


annotate_hotspot_layers = (
    get_gdf_geom_type.set_task_instance_id("annotate_hotspot_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=reproject_hotspot_areas, **annotate_hotspot_layers_params)
    .call()
)


# %% [markdown]
# ## Annotate protected areas with geom type

# %%
# parameters

annotate_protected_layers_params = dict()

# %%
# call the task


annotate_protected_layers = (
    get_gdf_geom_type.set_task_instance_id("annotate_protected_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=reproject_protected_areas, **annotate_protected_layers_params)
    .call()
)


# %% [markdown]
# ## Create layer for amboseli group ranch

# %%
# parameters

custom_amboseli_layer_params = dict()

# %%
# call the task


custom_amboseli_layer = (
    create_deckgl_layer_from_gdf.set_task_instance_id("custom_amboseli_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=annotate_ambo_layers,
        style={
            "get_line_color": [169, 169, 169],
            "get_fill_color": [169, 169, 169],
            "get_line_width": 4.0,
            "opacity": 0.75,
            "extruded": False,
            "stroked": True,
            "filled": False,
        },
        legend={
            "title": "Map layers",
            "values": [{"label": "Group ranch boundaries", "color": "#a9a9a9"}],
        },
        **custom_amboseli_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Create layer for hotspot areas

# %%
# parameters

custom_hotspot_layer_params = dict()

# %%
# call the task


custom_hotspot_layer = (
    create_deckgl_layer_from_gdf.set_task_instance_id("custom_hotspot_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=annotate_hotspot_layers,
        style={
            "get_line_color": [220, 20, 60],
            "get_fill_color": [220, 20, 60],
            "get_radius": 6,
            "get_line_width": 1.95,
            "opacity": 0.95,
            "extruded": False,
            "stroked": True,
            "filled": True,
        },
        legend={
            "title": "",
            "values": [{"label": "Hotspot areas", "color": "#dc143c"}],
        },
        **custom_hotspot_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Create layer for protected areas

# %%
# parameters

custom_protected_layer_params = dict()

# %%
# call the task


custom_protected_layer = (
    create_deckgl_layer_from_gdf.set_task_instance_id("custom_protected_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=annotate_protected_layers,
        style={
            "get_line_color": [77, 102, 0],
            "get_fill_color": [77, 102, 0],
            "get_line_width": 1.95,
            "opacity": 0.15,
            "extruded": False,
            "stroked": True,
            "filled": True,
        },
        legend={
            "title": "",
            "values": [{"label": "National parks and reserves", "color": "#4d6600"}],
        },
        **custom_protected_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Create text layer for conflict hotspots

# %%
# parameters

create_hotspot_text_layer_params = dict()

# %%
# call the task


create_hotspot_text_layer = (
    create_custom_text_layer.set_task_instance_id("create_hotspot_text_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        geodataframe=reproject_hotspot_areas,
        layer_style={
            "get_text": "name",
            "get_color": [0, 0, 0, 255],
            "get_size": 1500,
            "size_units": "meters",
            "size_min_pixels": 65,
            "size_max_pixels": 100,
            "size_scale": 2.05,
            "font_family": "Calibri",
            "font_weight": "700",
            "get_text_anchor": "middle",
            "get_alignment_baseline": "center",
            "billboard": True,
            "background_padding": [4, 8],
            "pickable": True,
            "auto_highlight": False,
        },
        use_centroid=True,
        legend=None,
        **create_hotspot_text_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Get Subject Group observations from EarthRanger

# %%
# parameters

subject_obs_params = dict(
    subject_group_name=...,
)

# %%
# call the task


subject_obs = (
    get_subjectgroup_observations.set_task_instance_id("subject_obs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=er_client_name,
        time_range=time_range,
        raise_on_empty=False,
        include_details=False,
        include_subjectsource_details=False,
        **subject_obs_params,
    )
    .call()
)


# %% [markdown]
# ## Transform observations to relocations

# %%
# parameters

subject_reloc_params = dict()

# %%
# call the task


subject_reloc = (
    process_relocations.set_task_instance_id("subject_reloc")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        observations=subject_obs,
        relocs_columns=[
            "groupby_col",
            "fixtime",
            "junk_status",
            "geometry",
            "extra__subject__name",
            "extra__subject__subject_subtype",
            "extra__subject__sex",
        ],
        filter_point_coords=[
            {"x": 180.0, "y": 90.0},
            {"x": 0.0, "y": 0.0},
            {"x": 1.0, "y": 1.0},
        ],
        **subject_reloc_params,
    )
    .call()
)


# %% [markdown]
# ## Persist relocations

# %%
# parameters

persist_relocs_params = dict()

# %%
# call the task


persist_relocs = (
    persist_df.set_task_instance_id("persist_relocs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename="relocations",
        df=subject_reloc,
        **persist_relocs_params,
    )
    .call()
)


# %% [markdown]
# ## Transform relocations to trajectories

# %%
# parameters

subject_traj_params = dict(
    trajectory_segment_filter=...,
)

# %%
# call the task


subject_traj = (
    relocations_to_trajectory.set_task_instance_id("subject_traj")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(relocations=subject_reloc, **subject_traj_params)
    .call()
)


# %% [markdown]
# ## Persist trajectories

# %%
# parameters

persist_trajs_params = dict()

# %%
# call the task


persist_trajs = (
    persist_df.set_task_instance_id("persist_trajs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename="trajectories",
        df=subject_traj,
        **persist_trajs_params,
    )
    .call()
)


# %% [markdown]
# ## Add temporal index to Subject Trajectories

# %%
# parameters

traj_add_temporal_index_params = dict()

# %%
# call the task


traj_add_temporal_index = (
    add_temporal_index.set_task_instance_id("traj_add_temporal_index")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=subject_traj,
        time_col="segment_start",
        groupers=groupers,
        cast_to_datetime=True,
        format="mixed",
        **traj_add_temporal_index_params,
    )
    .call()
)


# %% [markdown]
# ## Classify trajectories by speed

# %%
# parameters

classify_trajectories_speed_bins_params = dict()

# %%
# call the task


classify_trajectories_speed_bins = (
    apply_classification.set_task_instance_id("classify_trajectories_speed_bins")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=traj_add_temporal_index,
        input_column_name="speed_kmhr",
        output_column_name="speed_bins",
        classification_options={"scheme": "equal_interval", "k": 6},
        label_options={"label_range": False, "label_decimals": 1},
        **classify_trajectories_speed_bins_params,
    )
    .call()
)


# %% [markdown]
# ## Rename trajectory columnns

# %%
# parameters

rename_traj_cols_params = dict()

# %%
# call the task


rename_traj_cols = (
    map_columns.set_task_instance_id("rename_traj_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "extra__name": "subject_name",
            "extra__sex": "subject_sex",
            "extra__subject_subtype": "subject_subtype",
        },
        df=classify_trajectories_speed_bins,
        **rename_traj_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Split subject trajectories by group

# %%
# parameters

split_subject_traj_groups_params = dict()

# %%
# call the task


split_subject_traj_groups = (
    split_groups.set_task_instance_id("split_subject_traj_groups")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=rename_traj_cols, groupers=groupers, **split_subject_traj_groups_params)
    .call()
)


# %% [markdown]
# ##

# %%
# parameters

td_params = dict(
    auto_scale_or_custom_cell_size=...,
    max_speed_factor=...,
    expansion_factor=...,
)

# %%
# call the task


td = (
    calculate_elliptical_time_density.set_task_instance_id("td")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
        nodata_value="nan",
        band_count=1,
        **td_params,
    )
    .mapvalues(argnames=["trajectory_gdf"], argvalues=split_subject_traj_groups)
)


# %% [markdown]
# ## Time density colormap

# %%
# parameters

td_colormap_params = dict()

# %%
# call the task


td_colormap = (
    apply_color_map.set_task_instance_id("td_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="percentile",
        colormap="RdYlGn",
        output_column_name="percentile_colormap",
        **td_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=td)
)


# %% [markdown]
# ## Create map layer from time density

# %%
# parameters

td_map_layer_params = dict()

# %%
# call the task


td_map_layer = (
    create_geojson_layer.set_task_instance_id("td_map_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_fill_color": "percentile_colormap",
            "opacity": 0.45,
            "get_line_width": 0.75,
            "stroked": True,
        },
        legend={"label_column": "percentile", "color_column": "percentile_colormap"},
        **td_map_layer_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=td_colormap)
)


# %% [markdown]
# ## Combine map layers

# %%
# parameters

combine_custom_map_layers_params = dict()

# %%
# call the task


combine_custom_map_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combine_custom_map_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[
            custom_amboseli_layer,
            custom_hotspot_layer,
            custom_protected_layer,
            create_hotspot_text_layer,
        ],
        **combine_custom_map_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=td_map_layer)
)


# %% [markdown]
# ## zoom by view state

# %%
# parameters

zoom_view_state_params = dict()

# %%
# call the task


zoom_view_state = (
    view_state_deck_gdf.set_task_instance_id("zoom_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(pitch=0, bearing=0, **zoom_view_state_params)
    .mapvalues(argnames=["gdf"], argvalues=td_colormap)
)


# %% [markdown]
# ## Zip layers and viewstate

# %%
# parameters

zip_layers_view_params = dict()

# %%
# call the task


zip_layers_view = (
    zip_groupbykey.set_task_instance_id("zip_layers_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combine_custom_map_layers, zoom_view_state], **zip_layers_view_params
    )
    .call()
)


# %% [markdown]
# ## Draw ecomap from time density

# %%
# parameters

td_ecomap_params = dict(
    widget_id=...,
)

# %%
# call the task


td_ecomap = (
    draw_map.set_task_instance_id("td_ecomap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=base_map_defs,
        static=False,
        title=None,
        max_zoom=15,
        legend_style={"placement": "bottom-right", "title": "ETD Metrics"},
        **td_ecomap_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zip_layers_view)
)


# %% [markdown]
# ## Persist ecomap as text

# %%
# parameters

td_ecomap_html_url_params = dict(
    filename=...,
)

# %%
# call the task


td_ecomap_html_url = (
    persist_text.set_task_instance_id("td_ecomap_html_url")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="homerange",
        **td_ecomap_html_url_params,
    )
    .mapvalues(argnames=["text"], argvalues=td_ecomap)
)


# %% [markdown]
# ## Create time density map widget

# %%
# parameters

td_map_widget_params = dict()

# %%
# call the task


td_map_widget = (
    create_map_widget_single_view.set_task_instance_id("td_map_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Home Range Metrics", **td_map_widget_params)
    .map(argnames=["view", "data"], argvalues=td_ecomap_html_url)
)


# %% [markdown]
# ## Merge Time Density Map Widget Views

# %%
# parameters

td_grouped_map_widget_params = dict()

# %%
# call the task


td_grouped_map_widget = (
    merge_widget_views.set_task_instance_id("td_grouped_map_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=td_map_widget, **td_grouped_map_widget_params)
    .call()
)


# %% [markdown]
# ## Generate summary table metrics for subjects

# %%
# parameters

summary_table_params = dict()

# %%
# call the task


summary_table = (
    summarize_df.set_task_instance_id("summary_table")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        groupby_cols=["subject_name"],
        summary_params=[
            {
                "display_name": "mean_speed",
                "aggregator": "mean",
                "column": "speed_kmhr",
                "decimal_places": 2,
            },
            {
                "display_name": "min_speed",
                "aggregator": "min",
                "column": "speed_kmhr",
                "decimal_places": 2,
            },
            {
                "display_name": "max_speed",
                "aggregator": "max",
                "column": "speed_kmhr",
                "decimal_places": 2,
            },
            {
                "display_name": "total_distance",
                "aggregator": "sum",
                "column": "dist_meters",
                "original_unit": "m",
                "new_unit": "km",
                "decimal_places": 2,
            },
        ],
        reset_index=True,
        **summary_table_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
)


# %% [markdown]
# ## Add total row on summary table

# %%
# parameters

add_total_events_row_params = dict()

# %%
# call the task


add_total_events_row = (
    add_totals_row.set_task_instance_id("add_total_events_row")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(label_col=["subject_name"], label="Total", **add_total_events_row_params)
    .mapvalues(argnames=["df"], argvalues=summary_table)
)


# %% [markdown]
# ## Persist summary table

# %%
# parameters

persist_summary_table_params = dict()

# %%
# call the task


persist_summary_table = (
    persist_df.set_task_instance_id("persist_summary_table")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename=None,
        **persist_summary_table_params,
    )
    .mapvalues(argnames=["df"], argvalues=add_total_events_row)
)


# %% [markdown]
# ## Convert collared subject html to png

# %%
# parameters

collared_html_png_params = dict()

# %%
# call the task


collared_html_png = (
    html_to_png.set_task_instance_id("collared_html_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 20000,
            "max_concurrent_pages": 1,
        },
        **collared_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=td_ecomap_html_url)
)


# %% [markdown]
# ## Sort trajectories by speed bins

# %%
# parameters

sort_trajs_by_speed_params = dict()

# %%
# call the task


sort_trajs_by_speed = (
    sort_values.set_task_instance_id("sort_trajs_by_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="speed_bins",
        na_position="first",
        ascending=True,
        **sort_trajs_by_speed_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_subject_traj_groups)
)


# %% [markdown]
# ## Apply colormap to speed bins

# %%
# parameters

apply_speed_colormap_params = dict()

# %%
# call the task


apply_speed_colormap = (
    apply_color_map.set_task_instance_id("apply_speed_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="speed_bins",
        output_column_name="speed_bins_colormap",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        **apply_speed_colormap_params,
    )
    .mapvalues(argnames=["df"], argvalues=sort_trajs_by_speed)
)


# %% [markdown]
# ## Format speed bins for legend

# %%
# parameters

format_speed_bin_labels_params = dict()

# %%
# call the task


format_speed_bin_labels = (
    map_values_with_unit.set_task_instance_id("format_speed_bin_labels")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="speed_bins",
        output_column_name="speed_bins_formatted",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        **format_speed_bin_labels_params,
    )
    .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
)


# %% [markdown]
# ## Format speed values for display

# %%
# parameters

format_speed_values_params = dict()

# %%
# call the task


format_speed_values = (
    map_values_with_unit.set_task_instance_id("format_speed_values")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="speed_kmhr",
        output_column_name="speed_kmhr",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        **format_speed_values_params,
    )
    .mapvalues(argnames=["df"], argvalues=format_speed_bin_labels)
)


# %% [markdown]
# ## Generate speedmap layers

# %%
# parameters

generate_speedmap_layers_params = dict()

# %%
# call the task


generate_speedmap_layers = (
    create_path_layer.set_task_instance_id("generate_speedmap_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_color": "speed_bins_colormap",
            "get_width": 2.85,
            "width_scale": 1,
            "width_min_pixels": 2,
            "width_max_pixels": 8,
            "width_units": "pixels",
            "cap_rounded": True,
            "joint_rounded": True,
            "billboard": False,
            "opacity": 0.55,
            "stroked": True,
        },
        legend={
            "title": "Speed (km/h)",
            "label_column": "speed_bins_formatted",
            "color_column": "speed_bins_colormap",
            "sort": "ascending",
            "label_suffix": None,
        },
        **generate_speedmap_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=format_speed_values)
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

zoom_speed_gdf_extent_params = dict()

# %%
# call the task


zoom_speed_gdf_extent = (
    view_state_deck_gdf.set_task_instance_id("zoom_speed_gdf_extent")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(pitch=0, bearing=0, **zoom_speed_gdf_extent_params)
    .mapvalues(argnames=["gdf"], argvalues=format_speed_values)
)


# %% [markdown]
# ## Combine map layers

# %%
# parameters

combine_speed_layers_params = dict()

# %%
# call the task


combine_speed_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combine_speed_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[
            custom_amboseli_layer,
            custom_hotspot_layer,
            custom_protected_layer,
            create_hotspot_text_layer,
        ],
        **combine_speed_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
)


# %% [markdown]
# ## Zip layers and viewstate

# %%
# parameters

zip_speed_layers_view_params = dict()

# %%
# call the task


zip_speed_layers_view = (
    zip_groupbykey.set_task_instance_id("zip_speed_layers_view")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combine_speed_layers, zoom_speed_gdf_extent],
        **zip_speed_layers_view_params,
    )
    .call()
)


# %% [markdown]
# ## Draw speedmap

# %%
# parameters

draw_speedmap_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_speedmap = (
    draw_map.set_task_instance_id("draw_speedmap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=base_map_defs,
        static=False,
        title=None,
        max_zoom=15,
        legend_style={"placement": "bottom-right"},
        **draw_speedmap_params,
    )
    .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zip_speed_layers_view)
)


# %% [markdown]
# ## Persist speedmap as text

# %%
# parameters

speedmap_html_url_params = dict(
    filename=...,
)

# %%
# call the task


speedmap_html_url = (
    persist_text.set_task_instance_id("speedmap_html_url")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename_suffix="speedmap",
        **speedmap_html_url_params,
    )
    .mapvalues(argnames=["text"], argvalues=draw_speedmap)
)


# %% [markdown]
# ## Convert speedmap html to png

# %%
# parameters

speed_html_png_params = dict()

# %%
# call the task


speed_html_png = (
    html_to_png.set_task_instance_id("speed_html_png")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 20000,
            "max_concurrent_pages": 1,
        },
        **speed_html_png_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=speedmap_html_url)
)


# %% [markdown]
# ## Create speedmap widget

# %%
# parameters

speedmap_widget_params = dict()

# %%
# call the task


speedmap_widget = (
    create_map_widget_single_view.set_task_instance_id("speedmap_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Speedmap", **speedmap_widget_params)
    .map(argnames=["view", "data"], argvalues=speedmap_html_url)
)


# %% [markdown]
# ## Merge speedmap widget view

# %%
# parameters

sm_grouped_map_widget_params = dict()

# %%
# call the task


sm_grouped_map_widget = (
    merge_widget_views.set_task_instance_id("sm_grouped_map_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=speedmap_widget, **sm_grouped_map_widget_params)
    .call()
)


# %% [markdown]
# ## Calculate mean speed

# %%
# parameters

calc_mean_speed_params = dict()

# %%
# call the task


calc_mean_speed = (
    dataframe_column_sum.set_task_instance_id("calc_mean_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="mean_speed", **calc_mean_speed_params)
    .mapvalues(argnames=["df"], argvalues=summary_table)
)


# %% [markdown]
# ## Round off mean speed to 2 decimal_places

# %%
# parameters

round_mean_speed_params = dict()

# %%
# call the task


round_mean_speed = (
    round_off_values.set_task_instance_id("round_mean_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(dp=2, **round_mean_speed_params)
    .mapvalues(argnames=["value"], argvalues=calc_mean_speed)
)


# %% [markdown]
# ## Calculate min speed

# %%
# parameters

calc_min_speed_params = dict()

# %%
# call the task


calc_min_speed = (
    dataframe_column_sum.set_task_instance_id("calc_min_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="min_speed", **calc_min_speed_params)
    .mapvalues(argnames=["df"], argvalues=summary_table)
)


# %% [markdown]
# ## Round off min speed to 2 decimal_places

# %%
# parameters

round_min_speed_params = dict()

# %%
# call the task


round_min_speed = (
    round_off_values.set_task_instance_id("round_min_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(dp=2, **round_min_speed_params)
    .mapvalues(argnames=["value"], argvalues=calc_min_speed)
)


# %% [markdown]
# ## Calculate max speed

# %%
# parameters

calc_max_speed_params = dict()

# %%
# call the task


calc_max_speed = (
    dataframe_column_sum.set_task_instance_id("calc_max_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="max_speed", **calc_max_speed_params)
    .mapvalues(argnames=["df"], argvalues=summary_table)
)


# %% [markdown]
# ## Round off max speed to 2 decimal_places

# %%
# parameters

round_max_speed_params = dict()

# %%
# call the task


round_max_speed = (
    round_off_values.set_task_instance_id("round_max_speed")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(dp=2, **round_max_speed_params)
    .mapvalues(argnames=["value"], argvalues=calc_max_speed)
)


# %% [markdown]
# ## Calculate total distance

# %%
# parameters

total_distance_covered_params = dict()

# %%
# call the task


total_distance_covered = (
    dataframe_column_sum.set_task_instance_id("total_distance_covered")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="total_distance", **total_distance_covered_params)
    .mapvalues(argnames=["df"], argvalues=summary_table)
)


# %% [markdown]
# ## Round off total distance to 2 decimal_places

# %%
# parameters

round_total_distance_params = dict()

# %%
# call the task


round_total_distance = (
    round_off_values.set_task_instance_id("round_total_distance")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(dp=2, **round_total_distance_params)
    .mapvalues(argnames=["value"], argvalues=total_distance_covered)
)


# %% [markdown]
# ## Unique subjects on trajs

# %%
# parameters

unique_subjects_params = dict()

# %%
# call the task


unique_subjects = (
    dataframe_column_nunique.set_task_instance_id("unique_subjects")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=traj_add_temporal_index, column_name="groupby_col", **unique_subjects_params
    )
    .call()
)


# %% [markdown]
# ## Create cover template context

# %%
# parameters

create_cover_tpl_context_params = dict()

# %%
# call the task


create_cover_tpl_context = (
    create_cl_ctx_cover.set_task_instance_id("create_cover_tpl_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        count=unique_subjects,
        report_period=time_range,
        prepared_by="Ecoscope",
        **create_cover_tpl_context_params,
    )
    .call()
)


# %% [markdown]
# ## Persist cover page context

# %%
# parameters

persist_cover_context_params = dict()

# %%
# call the task


persist_cover_context = (
    create_context_page_lg.set_task_instance_id("persist_cover_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        template_path=persist_cover_page,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context=create_cover_tpl_context,
        filename="lg_cover_page.docx",
        **persist_cover_context_params,
    )
    .call()
)


# %% [markdown]
# ## Group grouper context values together

# %%
# parameters

group_context_values_params = dict()

# %%
# call the task


group_context_values = (
    zip_groupbykey.set_task_instance_id("group_context_values")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[
            split_subject_traj_groups,
            round_total_distance,
            collared_html_png,
            speed_html_png,
        ],
        **group_context_values_params,
    )
    .call()
)


# %% [markdown]
# ## Create individual grouper context

# %%
# parameters

indv_cl_ctx_params = dict()

# %%
# call the task


indv_cl_ctx = (
    create_collared_lions_grouper_ctx.set_task_instance_id("indv_cl_ctx")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(grouper_name=groupers, **indv_cl_ctx_params)
    .mapvalues(
        argnames=["df", "total_distance", "home_range", "speed_map"],
        argvalues=group_context_values,
    )
)


# %% [markdown]
# ## Create grouper word doc

# %%
# parameters

create_grouper_doc_params = dict()

# %%
# call the task


create_grouper_doc = (
    create_grouper_page.set_task_instance_id("create_grouper_doc")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        template_path=persist_indv_subject_page,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename=None,
        validate_images=True,
        box_h_cm=6.5,
        box_w_cm=11.11,
        **create_grouper_doc_params,
    )
    .mapvalues(argnames=["context"], argvalues=indv_cl_ctx)
)


# %% [markdown]
# ## Merge word docs

# %%
# parameters

merge_docx_params = dict()

# %%
# call the task


merge_docx = (
    merge_cl_files.set_task_instance_id("merge_docx")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        cover_page_path=persist_cover_context,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context_page_items=create_grouper_doc,
        filename=None,
        **merge_docx_params,
    )
    .call()
)


# %% [markdown]
# ## Create single value widgets for total mean speed per group

# %%
# parameters

total_mean_speed_widgets_params = dict()

# %%
# call the task


total_mean_speed_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_mean_speed_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Mean Speed", decimal_places=2, **total_mean_speed_widgets_params)
    .map(argnames=["view", "data"], argvalues=round_mean_speed)
)


# %% [markdown]
# ## Merge per group mean speed SV widgets

# %%
# parameters

total_mean_speed_sv_widget_params = dict()

# %%
# call the task


total_mean_speed_sv_widget = (
    merge_widget_views.set_task_instance_id("total_mean_speed_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_mean_speed_widgets, **total_mean_speed_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Create single value Widgets for total min speed per group

# %%
# parameters

total_min_speed_widgets_params = dict()

# %%
# call the task


total_min_speed_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_min_speed_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Min Speed", decimal_places=2, **total_min_speed_widgets_params)
    .map(argnames=["view", "data"], argvalues=round_min_speed)
)


# %% [markdown]
# ## Merge per group min speed SV widgets

# %%
# parameters

total_min_speed_sv_widget_params = dict()

# %%
# call the task


total_min_speed_sv_widget = (
    merge_widget_views.set_task_instance_id("total_min_speed_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_min_speed_widgets, **total_min_speed_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Create single value widgets for total max speed per group

# %%
# parameters

total_max_speed_widgets_params = dict()

# %%
# call the task


total_max_speed_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_max_speed_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(title="Max Speed", decimal_places=2, **total_max_speed_widgets_params)
    .map(argnames=["view", "data"], argvalues=round_max_speed)
)


# %% [markdown]
# ## Merge per group max speed SV widgets

# %%
# parameters

total_max_speed_sv_widget_params = dict()

# %%
# call the task


total_max_speed_sv_widget = (
    merge_widget_views.set_task_instance_id("total_max_speed_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_max_speed_widgets, **total_max_speed_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Create single value widgets for total distance per group

# %%
# parameters

total_distance_widgets_params = dict()

# %%
# call the task


total_distance_widgets = (
    create_single_value_widget_single_view.set_task_instance_id(
        "total_distance_widgets"
    )
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            never,
        ],
        unpack_depth=1,
    )
    .partial(
        title="Distance covered", decimal_places=2, **total_distance_widgets_params
    )
    .map(argnames=["view", "data"], argvalues=round_total_distance)
)


# %% [markdown]
# ## Merge per group total distance SV widgets

# %%
# parameters

total_distance_sv_widget_params = dict()

# %%
# call the task


total_distance_sv_widget = (
    merge_widget_views.set_task_instance_id("total_distance_sv_widget")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(widgets=total_distance_widgets, **total_distance_sv_widget_params)
    .call()
)


# %% [markdown]
# ## Lion Guardians dashboard

# %%
# parameters

lg_dashboard_params = dict(
    warning=...,
)

# %%
# call the task


lg_dashboard = (
    gather_dashboard.set_task_instance_id("lg_dashboard")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        details=workflow_details,
        widgets=[
            total_mean_speed_sv_widget,
            total_min_speed_sv_widget,
            total_max_speed_sv_widget,
            total_distance_sv_widget,
            td_grouped_map_widget,
            sm_grouped_map_widget,
        ],
        time_range=time_range,
        groupers=groupers,
        **lg_dashboard_params,
    )
    .call()
)
